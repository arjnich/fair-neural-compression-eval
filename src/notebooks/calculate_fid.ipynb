{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "device = torch.device(f'cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "fid = FrechetInceptionDistance(feature=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celebA\n",
      "fairface\n"
     ]
    }
   ],
   "source": [
    "dir = '/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/mbt2018'\n",
    "for name in os.listdir(dir):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_path = '/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/jpeg/q_1'\n",
    "meta_data_path = '/media/global_data/fair_neural_compression_data/datasets/RFW/clean_metadata/numerical_labels.csv'\n",
    "clean_images_path = '/media/global_data/fair_neural_compression_data/datasets/RFW/data_64'\n",
    "\n",
    "meta_data_inf = pd.read_csv(meta_data_path).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(meta_data_inf, path):\n",
    "    image_tensors = []\n",
    "    for meta_data in tqdm(meta_data_inf, total=len(meta_data_inf)):\n",
    "        file_path = os.path.join(path, meta_data[2], meta_data[1])\n",
    "        image = Image.open(file_path).convert('RGB')\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensors.append(image_tensor)\n",
    "    \n",
    "    image_tensors = torch.stack(image_tensors, dim=0)\n",
    "    return image_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40607/40607 [00:08<00:00, 4756.98it/s]\n"
     ]
    }
   ],
   "source": [
    "generated_image_tensors = get_images(meta_data_inf, generated_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image_tensors = generated_image_tensors.to(device).to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40607/40607 [00:10<00:00, 3957.30it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_image_tensors = get_images(meta_data_inf, clean_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_image_tensors = clean_image_tensors.to(device).to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fid_in_batches(fid, images, batch_size=128, real=True):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i + batch_size]\n",
    "        fid.update(batch, real=real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_fid_in_batches(fid, clean_image_tensors, batch_size=64, real=True)\n",
    "update_fid_in_batches(fid, generated_image_tensors, batch_size=64, real=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4183015082380734e-05"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(fid.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mbt2018\n",
    "    - celebA\n",
    "        - q0001: 6.5600e-05\n",
    "        - q0009: 2.8195e-05\n",
    "        - q1:\n",
    "        - q2:\n",
    "        - q3: 1.9363e-05\n",
    "        \n",
    "    - fairface\n",
    "        - q0001: 9.0552e-05\n",
    "        - q0009: 1.4183e-05\n",
    "        - q1: \n",
    "        - q2:\n",
    "        - q3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasta/.conda/envs/fnc_eval/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rasta/.conda/envs/fnc_eval/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = inception_v3(pretrained=True, transform_input=False)\n",
    "model.fc = torch.nn.Identity()  # Remove the final classification layer\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = preprocess(img)\n",
    "    return img.unsqueeze(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnc_eval",
   "language": "python",
   "name": "fnc_eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
