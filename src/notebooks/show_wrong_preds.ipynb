{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.v2 as transformsv2\n",
    "import sys\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "RESNET18_HEIGHT = 224\n",
    "RESNET18_WIDTH = 224\n",
    "\n",
    "class RFW(Dataset):\n",
    "\n",
    "    def __init__(self, img_path, attr_path, transforms, png):\n",
    "\n",
    "        self.attr = pd.read_csv(attr_path).to_numpy()\n",
    "        print(f'attr: {self.attr}')\n",
    "        self.img_path = img_path\n",
    "        self.transforms = transforms\n",
    "        self.png = png\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.attr)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.png:\n",
    "            img =  read_image(os.path.join(self.img_path, self.attr[idx][2], self.attr[idx][1]))\n",
    "        else:\n",
    "            img =  read_image(os.path.join(self.img_path, self.attr[idx][2], self.attr[idx][1].replace(\"png\", \"jpg\")))\n",
    "        return self.transforms(img), torch.from_numpy(self.attr[idx][3:].astype(np.float32)),\\\n",
    "            self.attr[idx][2].split(\"/\")[0], f'{self.attr[idx][2]}/{self.attr[idx][1]}'\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    img_path, \n",
    "    attr_path, \n",
    "    batch_size, \n",
    "    train_test_ratio=0.7, \n",
    "    png=True, \n",
    "    seed=42\n",
    "):\n",
    "\n",
    "    tfs = transformsv2.Compose([\n",
    "        transformsv2.Resize((RESNET18_HEIGHT, RESNET18_WIDTH)),\n",
    "        transformsv2.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Create Dataset\n",
    "    data = RFW(img_path, attr_path, tfs, png)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    trainset_size = int(len(data) * train_test_ratio)\n",
    "    validaset_size = int((len(data) - trainset_size) * 0.5)\n",
    "    testset_size = len(data) - trainset_size - validaset_size\n",
    "\n",
    "    trainset, valset, testset = random_split(data, [trainset_size, validaset_size, testset_size], generator)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(trainset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size)\n",
    "    test_loader = DataLoader(testset, batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "RACE_LABELS = ['Indian', 'Asian', 'African', 'Caucasian']\n",
    "ROOT = '/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64'\n",
    "RFW_LABELS_DIR = \"/media/global_data/fair_neural_compression_data/datasets/RFW/clean_metadata/numerical_labels_sorted.csv\"\n",
    "RATIO = 0.8\n",
    "BATCH_SIZE = 32\n",
    "model_names = ['cheng2020-attn', 'hyperprior', 'mbt2018', 'qarv', 'qres17m']\n",
    "# attribites = ['']\n",
    "# hard coding model_name to be the cheng2020-attn model\n",
    "model_name = 'cheng2020-attn'\n",
    "datasets_names = ['fairface', 'celebA']\n",
    "dataset_name = 'celebA'\n",
    "\n",
    "qualities = ['q_0001', 'q_0009', 'q_1', 'q_2', 'q_3']\n",
    "# mapping model name to corresponding decoded image storage dir.\n",
    "qualities_dict = {'hyperprior':['q_0001', 'q_0009', 'q_1', 'q_2', 'q_3'], \n",
    "                  'mbt2018':['q_0001', 'q_0009', 'q_1', 'q_2', 'q_3'], \n",
    "                  'cheng2020-attn':['q_0001', 'q_0009', 'q_1', 'q_2', 'q_3'], \n",
    "                  'qres17m':['1', '3', '6', '9', '12'], \n",
    "                  'qarv':['lmb_1', 'lmb_4', 'lmb_8', 'lmb_16', 'lmb_32']}\n",
    "model_name_dict = {'hyperprior':'hyperprior', \n",
    "                  'mbt2018':'mbt2018', \n",
    "                  'cheng2020-attn':'cheng2020-attn', \n",
    "                  'qres17m':'qres17m_lmb_64', \n",
    "                  'qarv':'qarv'}\n",
    "\n",
    "for quality in qualities_dict[model_name]:\n",
    "    train_loader, valid_loader, test_loader = create_dataloaders(\n",
    "        f'{ROOT}/{model_name_dict[model_name]}/{dataset_name}/{quality}', \n",
    "        RFW_LABELS_DIR, \n",
    "        BATCH_SIZE, \n",
    "        RATIO\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_names = []\n",
    "for batch in test_loader:\n",
    "    test_file_names.extend(list(batch[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Caucasian/m.04vzp3/m.04vzp3_0005.png',\n",
       " 'Asian/m.06jvdd/m.06jvdd_0002.png',\n",
       " 'Caucasian/m.068p19/m.068p19_0002.png',\n",
       " 'African/m.09876j/m.09876j_0003.png',\n",
       " 'Caucasian/m.03f1sjy/m.03f1sjy_0002.png',\n",
       " 'Asian/m.02rzm97/m.02rzm97_0002.png',\n",
       " 'African/m.01mvw82/m.01mvw82_0003.png',\n",
       " 'Caucasian/m.0gbx3b6/m.0gbx3b6_0004.png',\n",
       " 'Indian/m.0h1sgs/m.0h1sgs_0003.png',\n",
       " 'African/m.01mjyj0/m.01mjyj0_0003.png',\n",
       " 'African/m.0j3vxkv/m.0j3vxkv_0002.png',\n",
       " 'African/m.026_y13/m.026_y13_0004.png',\n",
       " 'Caucasian/m.03plx_/m.03plx__0001.png',\n",
       " 'Asian/m.06c1fd/m.06c1fd_0004.png',\n",
       " 'Caucasian/m.04dyc2/m.04dyc2_0003.png',\n",
       " 'Caucasian/m.026ths5/m.026ths5_0002.png',\n",
       " 'Indian/m.060dvc/m.060dvc_0002.png',\n",
       " 'Indian/m.02qytnc/m.02qytnc_0001.png',\n",
       " 'Indian/m.025xfy6/m.025xfy6_0004.png',\n",
       " 'Asian/m.07pkn7/m.07pkn7_0002.png',\n",
       " 'African/m.03_w4m/m.03_w4m_0002.png',\n",
       " 'Indian/m.04q699/m.04q699_0001.png',\n",
       " 'African/m.04lxlw/m.04lxlw_0003.png',\n",
       " 'Indian/m.0c7wt7/m.0c7wt7_0001.png',\n",
       " 'Asian/m.0c6c6x/m.0c6c6x_0004.png',\n",
       " 'African/m.0kn7cv4/m.0kn7cv4_0002.png',\n",
       " 'Indian/m.08r_r8/m.08r_r8_0002.png',\n",
       " 'African/m.0fqpsr1/m.0fqpsr1_0003.png',\n",
       " 'Indian/m.0288m_h/m.0288m_h_0001.png',\n",
       " 'Asian/m.0ndsl0w/m.0ndsl0w_0003.png',\n",
       " 'Indian/m.0g54n2/m.0g54n2_0003.png',\n",
       " 'Indian/m.0g5t3r1/m.0g5t3r1_0001.png',\n",
       " 'Indian/m.029t5l/m.029t5l_0003.png',\n",
       " 'Asian/m.0c3x99w/m.0c3x99w_0003.png',\n",
       " 'African/m.0fql_9p/m.0fql_9p_0004.png',\n",
       " 'African/m.049v2h/m.049v2h_0003.png',\n",
       " 'Caucasian/m.01k15v1/m.01k15v1_0003.png',\n",
       " 'Indian/m.0ndwnjc/m.0ndwnjc_0003.png',\n",
       " 'African/m.026n5fh/m.026n5fh_0003.png',\n",
       " 'Indian/m.0520f23/m.0520f23_0001.png',\n",
       " 'Asian/m.036qjq/m.036qjq_0003.png',\n",
       " 'Indian/m.03h0bdk/m.03h0bdk_0003.png',\n",
       " 'Asian/m.0dcpsg/m.0dcpsg_0002.png',\n",
       " 'Caucasian/m.02pwbf1/m.02pwbf1_0006.png',\n",
       " 'African/m.01yrrh/m.01yrrh_0003.png',\n",
       " 'Asian/m.0dzpq2/m.0dzpq2_0001.png',\n",
       " 'Indian/m.05p7pq/m.05p7pq_0002.png',\n",
       " 'Caucasian/m.05c4mjl/m.05c4mjl_0001.png',\n",
       " 'Caucasian/m.05zmfwc/m.05zmfwc_0004.png',\n",
       " 'Indian/m.0fph9sg/m.0fph9sg_0003.png',\n",
       " 'Indian/m.0j9p5_f/m.0j9p5_f_0003.png',\n",
       " 'Indian/m.0fzxp3/m.0fzxp3_0001.png',\n",
       " 'Indian/m.0cyws4/m.0cyws4_0001.png',\n",
       " 'Indian/m.0fq2084/m.0fq2084_0003.png',\n",
       " 'Caucasian/m.05nhmn/m.05nhmn_0002.png',\n",
       " 'African/m.0463mm7/m.0463mm7_0002.png',\n",
       " 'African/m.06_w2w7/m.06_w2w7_0001.png',\n",
       " 'African/m.0d1v67/m.0d1v67_0002.png',\n",
       " 'Indian/m.0h_d32_/m.0h_d32__0002.png',\n",
       " 'Caucasian/m.0bk1gm/m.0bk1gm_0001.png',\n",
       " 'Indian/m.03cbz21/m.03cbz21_0002.png',\n",
       " 'Caucasian/m.0gsksn/m.0gsksn_0002.png',\n",
       " 'Caucasian/m.07h1f1/m.07h1f1_0004.png',\n",
       " 'Indian/m.09rvdzt/m.09rvdzt_0002.png',\n",
       " 'Asian/m.04glpb0/m.04glpb0_0002.png',\n",
       " 'African/m.0bmhmkz/m.0bmhmkz_0004.png',\n",
       " 'Indian/m.0g9txr9/m.0g9txr9_0002.png',\n",
       " 'Asian/m.0ftqr/m.0ftqr_0001.png',\n",
       " 'African/m.01kcys/m.01kcys_0001.png',\n",
       " 'Caucasian/m.01gpm2/m.01gpm2_0001.png',\n",
       " 'African/m.0805qgz/m.0805qgz_0003.png',\n",
       " 'Asian/m.0fzjhv/m.0fzjhv_0001.png',\n",
       " 'Asian/m.07dc34/m.07dc34_0002.png',\n",
       " 'African/m.0gq6_0/m.0gq6_0_0001.png',\n",
       " 'African/m.01n3_7f/m.01n3_7f_0002.png',\n",
       " 'Caucasian/m.0g_gmg/m.0g_gmg_0001.png',\n",
       " 'Indian/m.0hzrn7d/m.0hzrn7d_0003.png',\n",
       " 'Asian/m.09whr0/m.09whr0_0003.png',\n",
       " 'Indian/m.0274f0x/m.0274f0x_0003.png',\n",
       " 'African/m.05z2dc/m.05z2dc_0003.png',\n",
       " 'Indian/m.03gspzq/m.03gspzq_0003.png',\n",
       " 'Caucasian/m.01cy59/m.01cy59_0003.png',\n",
       " 'Caucasian/m.03mhgw_/m.03mhgw__0002.png',\n",
       " 'African/m.0ndxbxz/m.0ndxbxz_0001.png',\n",
       " 'Caucasian/m.03q4y7/m.03q4y7_0003.png',\n",
       " 'Caucasian/m.03nncsk/m.03nncsk_0002.png',\n",
       " 'Caucasian/m.04mx7s/m.04mx7s_0001.png',\n",
       " 'African/m.09m6mq/m.09m6mq_0004.png',\n",
       " 'Caucasian/m.0jsfs9k/m.0jsfs9k_0003.png',\n",
       " 'African/m.07yzk2/m.07yzk2_0001.png',\n",
       " 'Indian/m.0bjh1x/m.0bjh1x_0001.png',\n",
       " 'Caucasian/m.0ghbzh/m.0ghbzh_0006.png',\n",
       " 'African/m.03c31xs/m.03c31xs_0002.png',\n",
       " 'African/m.086djf/m.086djf_0002.png',\n",
       " 'Indian/m.0cm19f/m.0cm19f_0003.png',\n",
       " 'Asian/m.04q1vlw/m.04q1vlw_0002.png',\n",
       " 'African/m.046jwr/m.046jwr_0004.png',\n",
       " 'African/m.01f97r/m.01f97r_0001.png',\n",
       " 'African/m.05b4cc_/m.05b4cc__0003.png',\n",
       " 'African/m.07sbrxs/m.07sbrxs_0003.png',\n",
       " 'Caucasian/m.03d258z/m.03d258z_0002.png',\n",
       " 'Asian/m.0bmrv3/m.0bmrv3_0002.png',\n",
       " 'African/m.0fk1_z/m.0fk1_z_0002.png',\n",
       " 'Asian/m.02s_0f/m.02s_0f_0003.png',\n",
       " 'African/m.0cyw0s/m.0cyw0s_0002.png',\n",
       " 'Caucasian/m.0ch2qzm/m.0ch2qzm_0002.png',\n",
       " 'Indian/m.0dgnngj/m.0dgnngj_0001.png',\n",
       " 'Indian/m.03gzpz2/m.03gzpz2_0001.png',\n",
       " 'Asian/m.02z1pvy/m.02z1pvy_0001.png',\n",
       " 'African/m.05f3xb2/m.05f3xb2_0003.png',\n",
       " 'African/m.03zp0s/m.03zp0s_0002.png',\n",
       " 'Asian/m.02rfd99/m.02rfd99_0003.png',\n",
       " 'Asian/m.065zr88/m.065zr88_0004.png',\n",
       " 'Indian/m.0h3vyzy/m.0h3vyzy_0003.png',\n",
       " 'Caucasian/m.03gr2g0/m.03gr2g0_0001.png',\n",
       " 'Asian/m.02pm_cv/m.02pm_cv_0001.png',\n",
       " 'Caucasian/m.01x3p_/m.01x3p__0002.png',\n",
       " 'African/m.0kvhpcv/m.0kvhpcv_0004.png',\n",
       " 'African/m.07v6r9/m.07v6r9_0003.png',\n",
       " 'Asian/m.05hcpc/m.05hcpc_0001.png',\n",
       " 'Asian/m.022w7q/m.022w7q_0002.png',\n",
       " 'Caucasian/m.02j6cs/m.02j6cs_0001.png',\n",
       " 'African/m.0fqmz2h/m.0fqmz2h_0005.png',\n",
       " 'African/m.03s4p_/m.03s4p__0003.png',\n",
       " 'Indian/m.01pgck/m.01pgck_0001.png',\n",
       " 'African/m.0c3wt3n/m.0c3wt3n_0001.png',\n",
       " 'African/m.04tzx1/m.04tzx1_0001.png',\n",
       " 'Caucasian/m.026r446/m.026r446_0001.png',\n",
       " 'Caucasian/m.0bh9pqh/m.0bh9pqh_0002.png',\n",
       " 'African/m.04n2dlf/m.04n2dlf_0004.png',\n",
       " 'African/m.01twq8c/m.01twq8c_0001.png',\n",
       " 'Indian/m.08zncx/m.08zncx_0004.png',\n",
       " 'Indian/m.0f31j_/m.0f31j__0003.png',\n",
       " 'African/m.026d1r1/m.026d1r1_0001.png',\n",
       " 'Asian/m.0j62rxp/m.0j62rxp_0003.png',\n",
       " 'Indian/m.0b6j17m/m.0b6j17m_0002.png',\n",
       " 'African/m.05h9zw6/m.05h9zw6_0004.png',\n",
       " 'Caucasian/m.05v57d/m.05v57d_0004.png',\n",
       " 'African/m.01nxt09/m.01nxt09_0001.png',\n",
       " 'African/m.0bmcvtp/m.0bmcvtp_0002.png',\n",
       " 'African/m.01mfmqw/m.01mfmqw_0005.png',\n",
       " 'African/m.0559h3/m.0559h3_0001.png',\n",
       " 'African/m.031ycv/m.031ycv_0001.png',\n",
       " 'Caucasian/m.02qhwqv/m.02qhwqv_0002.png',\n",
       " 'Indian/m.0674cw/m.0674cw_0002.png',\n",
       " 'African/m.0fm0dg/m.0fm0dg_0001.png',\n",
       " 'African/m.0j7lb0k/m.0j7lb0k_0002.png',\n",
       " 'Caucasian/m.04vq3y/m.04vq3y_0003.png',\n",
       " 'African/m.02qxlg/m.02qxlg_0004.png',\n",
       " 'Asian/m.01bffy/m.01bffy_0003.png',\n",
       " 'Indian/m.054yrb/m.054yrb_0002.png',\n",
       " 'Caucasian/m.02vsjy0/m.02vsjy0_0007.png',\n",
       " 'Asian/m.0jt578s/m.0jt578s_0002.png',\n",
       " 'Caucasian/m.096bdr/m.096bdr_0001.png',\n",
       " 'Indian/m.0j0_j5z/m.0j0_j5z_0001.png',\n",
       " 'African/m.08hcd7/m.08hcd7_0003.png',\n",
       " 'Caucasian/m.05jyjn/m.05jyjn_0004.png',\n",
       " 'Asian/m.08kqw0/m.08kqw0_0003.png',\n",
       " 'Caucasian/m.03ms85/m.03ms85_0002.png',\n",
       " 'Caucasian/m.09v9q1w/m.09v9q1w_0002.png',\n",
       " 'Caucasian/m.01rq7g2/m.01rq7g2_0004.png',\n",
       " 'Caucasian/m.01_gcy/m.01_gcy_0004.png',\n",
       " 'African/m.027lgj5/m.027lgj5_0002.png',\n",
       " 'Indian/m.02r74r9/m.02r74r9_0003.png',\n",
       " 'Asian/m.04xvn3/m.04xvn3_0002.png',\n",
       " 'Indian/m.0czbv_w/m.0czbv_w_0001.png',\n",
       " 'Indian/m.0765gs/m.0765gs_0002.png',\n",
       " 'African/m.026cz6h/m.026cz6h_0001.png',\n",
       " 'Caucasian/m.0gwzd9y/m.0gwzd9y_0001.png',\n",
       " 'Caucasian/m.0fqq2n/m.0fqq2n_0002.png',\n",
       " 'Indian/m.027tqml/m.027tqml_0002.png',\n",
       " 'African/m.0kswqn/m.0kswqn_0003.png',\n",
       " 'Asian/m.0j2jqrf/m.0j2jqrf_0002.png',\n",
       " 'Caucasian/m.0c7phlk/m.0c7phlk_0003.png',\n",
       " 'African/m.0l8qhs_/m.0l8qhs__0004.png',\n",
       " 'Asian/m.06wblfx/m.06wblfx_0002.png',\n",
       " 'Caucasian/m.019s7b/m.019s7b_0003.png',\n",
       " 'Asian/m.026tg2j/m.026tg2j_0003.png',\n",
       " 'Indian/m.049442/m.049442_0003.png',\n",
       " 'Caucasian/m.0b2bxw/m.0b2bxw_0004.png',\n",
       " 'African/m.05m0xf/m.05m0xf_0001.png',\n",
       " 'Indian/m.04q9vbh/m.04q9vbh_0003.png',\n",
       " 'Indian/m.09gds74/m.09gds74_0002.png',\n",
       " 'Asian/m.09_nmy/m.09_nmy_0002.png',\n",
       " 'African/m.04y7mty/m.04y7mty_0001.png',\n",
       " 'Caucasian/m.09gk4md/m.09gk4md_0004.png',\n",
       " 'Indian/m.0271dpr/m.0271dpr_0004.png',\n",
       " 'Asian/m.02rs871/m.02rs871_0002.png',\n",
       " 'Caucasian/m.0q3_h32/m.0q3_h32_0001.png',\n",
       " 'Asian/m.03_53f/m.03_53f_0005.png',\n",
       " 'Indian/m.03csn8r/m.03csn8r_0002.png',\n",
       " 'African/m.064wkw/m.064wkw_0003.png',\n",
       " 'African/m.051zz11/m.051zz11_0003.png',\n",
       " 'Indian/m.027ztj2/m.027ztj2_0002.png',\n",
       " 'Indian/m.05t0fwn/m.05t0fwn_0002.png',\n",
       " 'Caucasian/m.07yz7f/m.07yz7f_0002.png',\n",
       " 'Asian/m.03c8qgj/m.03c8qgj_0001.png',\n",
       " 'Asian/m.09x34c/m.09x34c_0001.png',\n",
       " 'African/m.06zkxzn/m.06zkxzn_0001.png',\n",
       " 'African/m.0282hw2/m.0282hw2_0002.png',\n",
       " 'African/m.0457w0/m.0457w0_0001.png',\n",
       " 'Asian/m.0gyrngv/m.0gyrngv_0003.png',\n",
       " 'Asian/m.0g1tpl/m.0g1tpl_0003.png',\n",
       " 'Caucasian/m.028x00/m.028x00_0003.png',\n",
       " 'Caucasian/m.04z8sv/m.04z8sv_0004.png',\n",
       " 'Caucasian/m.070cmt/m.070cmt_0001.png',\n",
       " 'African/m.0g40qv/m.0g40qv_0002.png',\n",
       " 'Asian/m.02793mt/m.02793mt_0004.png',\n",
       " 'Caucasian/m.0611v_/m.0611v__0001.png',\n",
       " 'African/m.02_tv5/m.02_tv5_0001.png',\n",
       " 'African/m.01ld7zl/m.01ld7zl_0001.png',\n",
       " 'Asian/m.0fc2qq/m.0fc2qq_0002.png',\n",
       " 'Asian/m.0jt0gls/m.0jt0gls_0003.png',\n",
       " 'African/m.0c0ydn/m.0c0ydn_0002.png',\n",
       " 'Asian/m.0gmf_r_/m.0gmf_r__0002.png',\n",
       " 'Asian/m.0b3y217/m.0b3y217_0001.png',\n",
       " 'Caucasian/m.0cl3rj/m.0cl3rj_0003.png',\n",
       " 'Caucasian/m.02qf_k6/m.02qf_k6_0004.png',\n",
       " 'Caucasian/m.06gsg2/m.06gsg2_0002.png',\n",
       " 'Asian/m.04ggwj/m.04ggwj_0002.png',\n",
       " 'Indian/m.01t23c/m.01t23c_0001.png',\n",
       " 'Caucasian/m.02n10z/m.02n10z_0003.png',\n",
       " 'Indian/m.0gh7lmh/m.0gh7lmh_0003.png',\n",
       " 'Indian/m.0hgmx0n/m.0hgmx0n_0002.png',\n",
       " 'Asian/m.05tbhj/m.05tbhj_0005.png',\n",
       " 'Asian/m.0808_0r/m.0808_0r_0002.png',\n",
       " 'African/m.01tz35f/m.01tz35f_0001.png',\n",
       " 'African/m.029lw6/m.029lw6_0001.png',\n",
       " 'African/m.0kmg7qh/m.0kmg7qh_0001.png',\n",
       " 'Asian/m.05b598q/m.05b598q_0001.png',\n",
       " 'Caucasian/m.07bblg/m.07bblg_0002.png',\n",
       " 'Indian/m.05f5bt3/m.05f5bt3_0002.png',\n",
       " 'Indian/m.0cmd7dw/m.0cmd7dw_0001.png',\n",
       " 'Caucasian/m.072bxj/m.072bxj_0002.png',\n",
       " 'Asian/m.04h6fb/m.04h6fb_0002.png',\n",
       " 'African/m.076vxp/m.076vxp_0004.png',\n",
       " 'Asian/m.043pzvx/m.043pzvx_0002.png',\n",
       " 'African/m.072qwy/m.072qwy_0001.png',\n",
       " 'Caucasian/m.02r79x1/m.02r79x1_0004.png',\n",
       " 'Caucasian/m.0drbjd/m.0drbjd_0002.png',\n",
       " 'Indian/m.0j_5xy/m.0j_5xy_0001.png',\n",
       " 'African/m.0hnb7lp/m.0hnb7lp_0004.png',\n",
       " 'African/m.081z65/m.081z65_0001.png',\n",
       " 'African/m.025yd73/m.025yd73_0002.png',\n",
       " 'Asian/m.04zvnl5/m.04zvnl5_0002.png',\n",
       " 'African/m.04tz1l/m.04tz1l_0001.png',\n",
       " 'Asian/m.0bl734/m.0bl734_0003.png',\n",
       " 'Caucasian/m.09377v/m.09377v_0001.png',\n",
       " 'African/m.0d2z2g/m.0d2z2g_0003.png',\n",
       " 'Caucasian/m.079drpf/m.079drpf_0002.png',\n",
       " 'Asian/m.01rv1bs/m.01rv1bs_0002.png',\n",
       " 'Asian/m.0dyj83/m.0dyj83_0003.png',\n",
       " 'Asian/m.015vyh/m.015vyh_0002.png',\n",
       " 'Asian/m.02qxyx9/m.02qxyx9_0003.png',\n",
       " 'Indian/m.01wv4ry/m.01wv4ry_0003.png',\n",
       " 'Caucasian/m.098zc7/m.098zc7_0002.png',\n",
       " 'Asian/m.0gq572/m.0gq572_0002.png',\n",
       " 'African/m.03cfvqd/m.03cfvqd_0001.png',\n",
       " 'Caucasian/m.0240v/m.0240v_0005.png',\n",
       " 'African/m.026wvwc/m.026wvwc_0001.png',\n",
       " 'Asian/m.07dc34/m.07dc34_0003.png',\n",
       " 'Indian/m.0czf842/m.0czf842_0002.png',\n",
       " 'Caucasian/m.081lxk/m.081lxk_0002.png',\n",
       " 'Indian/m.02r5pn4/m.02r5pn4_0003.png',\n",
       " 'Asian/m.0h7lgs/m.0h7lgs_0003.png',\n",
       " 'Asian/m.044mrh/m.044mrh_0004.png',\n",
       " 'Indian/m.081hvm/m.081hvm_0003.png',\n",
       " 'Asian/m.0417khq/m.0417khq_0005.png',\n",
       " 'African/m.0b6lyjy/m.0b6lyjy_0003.png',\n",
       " 'Caucasian/m.0gjb1y4/m.0gjb1y4_0003.png',\n",
       " 'Caucasian/m.03vr4d/m.03vr4d_0004.png',\n",
       " 'African/m.048fq9/m.048fq9_0001.png',\n",
       " 'African/m.046t69/m.046t69_0003.png',\n",
       " 'Indian/m.02703x7/m.02703x7_0001.png',\n",
       " 'Asian/m.01yx4f/m.01yx4f_0002.png',\n",
       " 'Caucasian/m.07k8vcv/m.07k8vcv_0001.png',\n",
       " 'Caucasian/m.070_kf/m.070_kf_0001.png',\n",
       " 'African/m.02z2_6w/m.02z2_6w_0003.png',\n",
       " 'African/m.0d_thj/m.0d_thj_0002.png',\n",
       " 'Asian/m.04m7j2/m.04m7j2_0001.png',\n",
       " 'African/m.0dzlwj/m.0dzlwj_0001.png',\n",
       " 'African/m.07mmgx/m.07mmgx_0001.png',\n",
       " 'African/m.04y7mty/m.04y7mty_0002.png',\n",
       " 'Caucasian/m.01w923/m.01w923_0001.png',\n",
       " 'Asian/m.0ndskjk/m.0ndskjk_0003.png',\n",
       " 'African/m.05mk6s/m.05mk6s_0001.png',\n",
       " 'African/m.07jkvy/m.07jkvy_0004.png',\n",
       " 'Caucasian/m.09rscjd/m.09rscjd_0003.png',\n",
       " 'Caucasian/m.05xrf6/m.05xrf6_0002.png',\n",
       " 'Indian/m.0bpy2d/m.0bpy2d_0003.png',\n",
       " 'African/m.09mpd4/m.09mpd4_0001.png',\n",
       " 'Caucasian/m.03yly5c/m.03yly5c_0001.png',\n",
       " 'Asian/m.0bs7z4r/m.0bs7z4r_0001.png',\n",
       " 'Asian/m.08q_ll/m.08q_ll_0003.png',\n",
       " 'African/m.0bqpqh/m.0bqpqh_0003.png',\n",
       " 'African/m.0j9nkhb/m.0j9nkhb_0003.png',\n",
       " 'Caucasian/m.01vyt_1/m.01vyt_1_0003.png',\n",
       " 'Caucasian/m.06b4j5/m.06b4j5_0003.png',\n",
       " 'Asian/m.08gkz8/m.08gkz8_0003.png',\n",
       " 'Caucasian/m.04bnvr/m.04bnvr_0003.png',\n",
       " 'African/m.0pclyjm/m.0pclyjm_0004.png',\n",
       " 'Indian/m.04328m/m.04328m_0004.png',\n",
       " 'Asian/m.0cf93t/m.0cf93t_0002.png',\n",
       " 'Asian/m.026ftyd/m.026ftyd_0003.png',\n",
       " 'Indian/m.01hs26/m.01hs26_0003.png',\n",
       " 'African/m.01w0zk1/m.01w0zk1_0002.png',\n",
       " 'African/m.01kx43w/m.01kx43w_0002.png',\n",
       " 'Asian/m.043r3cy/m.043r3cy_0002.png',\n",
       " 'Asian/m.01t_v4b/m.01t_v4b_0001.png',\n",
       " 'African/m.01nk6j7/m.01nk6j7_0003.png',\n",
       " 'African/m.069g0m/m.069g0m_0003.png',\n",
       " 'Indian/m.0nbxy5g/m.0nbxy5g_0001.png',\n",
       " 'Caucasian/m.053qc6/m.053qc6_0002.png',\n",
       " 'African/m.08_0bw/m.08_0bw_0003.png',\n",
       " 'African/m.03t_38/m.03t_38_0002.png',\n",
       " 'African/m.07f3xb/m.07f3xb_0003.png',\n",
       " 'Asian/m.09g69fp/m.09g69fp_0002.png',\n",
       " 'Indian/m.03m9h0m/m.03m9h0m_0003.png',\n",
       " 'Indian/m.01w7n71/m.01w7n71_0001.png',\n",
       " 'Caucasian/m.04crbhv/m.04crbhv_0002.png',\n",
       " 'Asian/m.04yh64f/m.04yh64f_0004.png',\n",
       " 'Indian/m.03wf6kb/m.03wf6kb_0002.png',\n",
       " 'Asian/m.02qypmj/m.02qypmj_0003.png',\n",
       " 'African/m.0c02n3j/m.0c02n3j_0003.png',\n",
       " 'Indian/m.05f5b_y/m.05f5b_y_0002.png',\n",
       " 'Indian/m.0fq31yq/m.0fq31yq_0002.png',\n",
       " 'Indian/m.027f6m2/m.027f6m2_0003.png',\n",
       " 'Asian/m.03f4r_b/m.03f4r_b_0002.png',\n",
       " 'Asian/m.03r1d2/m.03r1d2_0002.png',\n",
       " 'African/m.0czcndq/m.0czcndq_0003.png',\n",
       " 'Caucasian/m.0by02cb/m.0by02cb_0003.png',\n",
       " 'Indian/m.02hks9/m.02hks9_0002.png',\n",
       " 'Caucasian/m.05hs7z/m.05hs7z_0002.png',\n",
       " 'Asian/m.01zdmw/m.01zdmw_0007.png',\n",
       " 'Indian/m.027q5vk/m.027q5vk_0002.png',\n",
       " 'Asian/m.09x34c/m.09x34c_0003.png',\n",
       " 'Indian/m.090gpr/m.090gpr_0002.png',\n",
       " 'African/m.0gkzpsn/m.0gkzpsn_0001.png',\n",
       " 'Indian/m.02qh579/m.02qh579_0001.png',\n",
       " 'Asian/m.0f1cgl/m.0f1cgl_0001.png',\n",
       " 'Caucasian/m.0bwcgt/m.0bwcgt_0001.png',\n",
       " 'Asian/m.02w_xk/m.02w_xk_0002.png',\n",
       " 'Indian/m.0fww52/m.0fww52_0002.png',\n",
       " 'Caucasian/m.07_k77/m.07_k77_0003.png',\n",
       " 'Caucasian/m.02k21g/m.02k21g_0006.png',\n",
       " 'Indian/m.0dd4jy/m.0dd4jy_0003.png',\n",
       " 'Caucasian/m.04znpf/m.04znpf_0002.png',\n",
       " 'African/m.0bbvbxy/m.0bbvbxy_0001.png',\n",
       " 'Caucasian/m.03m3_p0/m.03m3_p0_0005.png',\n",
       " 'Caucasian/m.026ths5/m.026ths5_0001.png',\n",
       " 'African/m.05_j1v/m.05_j1v_0001.png',\n",
       " 'Caucasian/m.0brpvb/m.0brpvb_0003.png',\n",
       " 'Caucasian/m.09v3wc5/m.09v3wc5_0002.png',\n",
       " 'Indian/m.02q7k9s/m.02q7k9s_0003.png',\n",
       " 'Caucasian/m.07rcm5/m.07rcm5_0001.png',\n",
       " 'Indian/m.0h3x88y/m.0h3x88y_0003.png',\n",
       " 'African/m.03npynb/m.03npynb_0003.png',\n",
       " 'Caucasian/m.01h3b9/m.01h3b9_0003.png',\n",
       " 'Asian/m.0gv5nz/m.0gv5nz_0004.png',\n",
       " 'Caucasian/m.0czc0z5/m.0czc0z5_0001.png',\n",
       " 'Asian/m.0bw148/m.0bw148_0002.png',\n",
       " 'Indian/m.02rylmb/m.02rylmb_0003.png',\n",
       " 'African/m.02x6910/m.02x6910_0005.png',\n",
       " 'Asian/m.088ysz/m.088ysz_0003.png',\n",
       " 'Asian/m.066ks8/m.066ks8_0001.png',\n",
       " 'Indian/m.02n1gr/m.02n1gr_0002.png',\n",
       " 'African/m.047frhh/m.047frhh_0003.png',\n",
       " 'African/m.01dqj6/m.01dqj6_0001.png',\n",
       " 'African/m.05zvsg3/m.05zvsg3_0002.png',\n",
       " 'Caucasian/m.0c2qjg/m.0c2qjg_0004.png',\n",
       " 'Indian/m.020sb7/m.020sb7_0002.png',\n",
       " 'Asian/m.05rtbg/m.05rtbg_0004.png',\n",
       " 'Asian/m.017j2s/m.017j2s_0002.png',\n",
       " 'Asian/m.0p9hf/m.0p9hf_0003.png',\n",
       " 'Indian/m.02wd63/m.02wd63_0002.png',\n",
       " 'Indian/m.09gds74/m.09gds74_0003.png',\n",
       " 'African/m.04zwqm6/m.04zwqm6_0003.png',\n",
       " 'Caucasian/m.03wfd02/m.03wfd02_0004.png',\n",
       " 'Indian/m.02qj6bs/m.02qj6bs_0001.png',\n",
       " 'Caucasian/m.0cl9kh/m.0cl9kh_0005.png',\n",
       " 'Caucasian/m.06l8mr/m.06l8mr_0002.png',\n",
       " 'African/m.052g6/m.052g6_0001.png',\n",
       " 'Indian/m.0djwkt/m.0djwkt_0001.png',\n",
       " 'Asian/m.0k3gbzs/m.0k3gbzs_0001.png',\n",
       " 'African/m.0bh7jyx/m.0bh7jyx_0004.png',\n",
       " 'Indian/m.0c2w5c/m.0c2w5c_0004.png',\n",
       " 'Asian/m.0g41bwm/m.0g41bwm_0002.png',\n",
       " 'Indian/m.09v4c6p/m.09v4c6p_0001.png',\n",
       " 'Asian/m.03cgdt5/m.03cgdt5_0002.png',\n",
       " 'Caucasian/m.01_yqg/m.01_yqg_0001.png',\n",
       " 'Caucasian/m.0278nmt/m.0278nmt_0003.png',\n",
       " 'African/m.02qpyql/m.02qpyql_0003.png',\n",
       " 'Indian/m.02z5jvt/m.02z5jvt_0001.png',\n",
       " 'Caucasian/m.0crk70/m.0crk70_0001.png',\n",
       " 'Caucasian/m.09jgcv/m.09jgcv_0002.png',\n",
       " 'Indian/m.03qcz13/m.03qcz13_0001.png',\n",
       " 'African/m.0nhr_g7/m.0nhr_g7_0003.png',\n",
       " 'Indian/m.05qtcv/m.05qtcv_0001.png',\n",
       " 'Asian/m.01xn37/m.01xn37_0001.png',\n",
       " 'Caucasian/m.023lrv/m.023lrv_0002.png',\n",
       " 'Asian/m.0lz2n/m.0lz2n_0004.png',\n",
       " 'African/m.018144/m.018144_0002.png',\n",
       " 'Caucasian/m.06zrlyg/m.06zrlyg_0002.png',\n",
       " 'Caucasian/m.03cnx89/m.03cnx89_0002.png',\n",
       " 'Asian/m.02qw5m/m.02qw5m_0001.png',\n",
       " 'African/m.07kj15/m.07kj15_0002.png',\n",
       " 'African/m.03d13n8/m.03d13n8_0003.png',\n",
       " 'Caucasian/m.03d16t/m.03d16t_0001.png',\n",
       " 'Indian/m.0807d6k/m.0807d6k_0002.png',\n",
       " 'Caucasian/m.02lyx4/m.02lyx4_0003.png',\n",
       " 'Asian/m.04yb6hs/m.04yb6hs_0003.png',\n",
       " 'African/m.027w0nt/m.027w0nt_0002.png',\n",
       " 'Caucasian/m.015yg9/m.015yg9_0001.png',\n",
       " 'Caucasian/m.0r4pmym/m.0r4pmym_0001.png',\n",
       " 'African/m.02r1g27/m.02r1g27_0003.png',\n",
       " 'Asian/m.061lnp/m.061lnp_0001.png',\n",
       " 'Caucasian/m.03bz3s8/m.03bz3s8_0003.png',\n",
       " 'Caucasian/m.043nftk/m.043nftk_0003.png',\n",
       " 'Indian/m.0bns8v/m.0bns8v_0003.png',\n",
       " 'Caucasian/m.0h55kdw/m.0h55kdw_0002.png',\n",
       " 'Caucasian/m.05zp1qt/m.05zp1qt_0002.png',\n",
       " 'Caucasian/m.02qck3r/m.02qck3r_0001.png',\n",
       " 'Asian/m.02w_xk/m.02w_xk_0001.png',\n",
       " 'Caucasian/m.0267_nc/m.0267_nc_0003.png',\n",
       " 'Asian/m.05q93_w/m.05q93_w_0002.png',\n",
       " 'Indian/m.06w9bxv/m.06w9bxv_0002.png',\n",
       " 'African/m.05b3qns/m.05b3qns_0004.png',\n",
       " 'African/m.0dp57c/m.0dp57c_0003.png',\n",
       " 'African/m.0crrqt/m.0crrqt_0004.png',\n",
       " 'African/m.01l5rfm/m.01l5rfm_0003.png',\n",
       " 'Indian/m.0jxbk4/m.0jxbk4_0001.png',\n",
       " 'Asian/m.03d1zfh/m.03d1zfh_0003.png',\n",
       " 'African/m.08ftv0/m.08ftv0_0003.png',\n",
       " 'African/m.09v3br0/m.09v3br0_0001.png',\n",
       " 'Caucasian/m.06yg_v/m.06yg_v_0002.png',\n",
       " 'Caucasian/m.0641fzk/m.0641fzk_0004.png',\n",
       " 'Asian/m.05mqp85/m.05mqp85_0004.png',\n",
       " 'Caucasian/m.0b0j6v/m.0b0j6v_0001.png',\n",
       " 'Caucasian/m.0h2ftf/m.0h2ftf_0002.png',\n",
       " 'Asian/m.0h55cwj/m.0h55cwj_0006.png',\n",
       " 'African/m.05f1l7/m.05f1l7_0003.png',\n",
       " 'Caucasian/m.0dsdqn0/m.0dsdqn0_0003.png',\n",
       " 'African/m.07bp7h/m.07bp7h_0004.png',\n",
       " 'African/m.02pk0cn/m.02pk0cn_0004.png',\n",
       " 'Caucasian/m.02flt3/m.02flt3_0003.png',\n",
       " 'Indian/m.0gkxvng/m.0gkxvng_0002.png',\n",
       " 'Caucasian/m.05y_48/m.05y_48_0004.png',\n",
       " 'Indian/m.04f0c97/m.04f0c97_0001.png',\n",
       " 'African/m.0gf30y/m.0gf30y_0001.png',\n",
       " 'Indian/m.02q4r5_/m.02q4r5__0003.png',\n",
       " 'Caucasian/m.048v2f/m.048v2f_0003.png',\n",
       " 'Indian/m.02r7h1g/m.02r7h1g_0004.png',\n",
       " 'Caucasian/m.01pvywj/m.01pvywj_0001.png',\n",
       " 'Indian/m.05f3bvm/m.05f3bvm_0003.png',\n",
       " 'Indian/m.02ry1s/m.02ry1s_0001.png',\n",
       " 'African/m.026w0np/m.026w0np_0003.png',\n",
       " 'African/m.03mcqys/m.03mcqys_0003.png',\n",
       " 'Asian/m.03d2pr0/m.03d2pr0_0002.png',\n",
       " 'African/m.0g79js/m.0g79js_0002.png',\n",
       " 'Caucasian/m.06m4k1/m.06m4k1_0001.png',\n",
       " 'Asian/m.04dt9z/m.04dt9z_0004.png',\n",
       " 'Caucasian/m.0642gkc/m.0642gkc_0001.png',\n",
       " 'African/m.0dnq7c/m.0dnq7c_0002.png',\n",
       " 'African/m.03f1pwv/m.03f1pwv_0003.png',\n",
       " 'African/m.02x5379/m.02x5379_0005.png',\n",
       " 'African/m.0kns6q7/m.0kns6q7_0002.png',\n",
       " 'African/m.08b0_7/m.08b0_7_0001.png',\n",
       " 'Indian/m.0bwc47/m.0bwc47_0004.png',\n",
       " 'African/m.01lmtl/m.01lmtl_0002.png',\n",
       " 'Asian/m.03nn7nh/m.03nn7nh_0004.png',\n",
       " 'Asian/m.0f4q6y/m.0f4q6y_0001.png',\n",
       " 'Indian/m.06zw_s/m.06zw_s_0003.png',\n",
       " 'Asian/m.06qfdm/m.06qfdm_0002.png',\n",
       " 'African/m.04ghf4p/m.04ghf4p_0003.png',\n",
       " 'Caucasian/m.070cmt/m.070cmt_0003.png',\n",
       " 'Asian/m.0hndd71/m.0hndd71_0003.png',\n",
       " 'Caucasian/m.0845nn/m.0845nn_0001.png',\n",
       " 'Asian/m.04jzfs/m.04jzfs_0001.png',\n",
       " 'African/m.02rr9vd/m.02rr9vd_0004.png',\n",
       " 'Indian/m.02wwms_/m.02wwms__0001.png',\n",
       " 'African/m.03c7lw_/m.03c7lw__0004.png',\n",
       " 'Asian/m.03k__w/m.03k__w_0001.png',\n",
       " 'African/m.03nqnfb/m.03nqnfb_0003.png',\n",
       " 'African/m.0n49v6t/m.0n49v6t_0002.png',\n",
       " 'Indian/m.07v2lm/m.07v2lm_0002.png',\n",
       " 'Caucasian/m.04q6ml4/m.04q6ml4_0003.png',\n",
       " 'Indian/m.03dctt/m.03dctt_0001.png',\n",
       " 'Caucasian/m.02d9q6/m.02d9q6_0002.png',\n",
       " 'African/m.0dqmwf/m.0dqmwf_0003.png',\n",
       " 'Caucasian/m.01c6dg/m.01c6dg_0002.png',\n",
       " 'Indian/m.0gc8tyv/m.0gc8tyv_0003.png',\n",
       " 'African/m.05r6qt/m.05r6qt_0002.png',\n",
       " 'Caucasian/m.09lpfq/m.09lpfq_0003.png',\n",
       " 'African/m.04dz7qh/m.04dz7qh_0004.png',\n",
       " 'African/m.03by2wy/m.03by2wy_0003.png',\n",
       " 'African/m.08kc30/m.08kc30_0002.png',\n",
       " 'Caucasian/m.08vrsq/m.08vrsq_0006.png',\n",
       " 'Asian/m.0c02l0/m.0c02l0_0004.png',\n",
       " 'Indian/m.01g4wh/m.01g4wh_0003.png',\n",
       " 'Indian/m.0j3fbcm/m.0j3fbcm_0002.png',\n",
       " 'African/m.03_wtr/m.03_wtr_0004.png',\n",
       " 'African/m.07bp7h/m.07bp7h_0003.png',\n",
       " 'Indian/m.0bh7qp1/m.0bh7qp1_0003.png',\n",
       " 'African/m.05y028/m.05y028_0005.png',\n",
       " 'Asian/m.01_v_j/m.01_v_j_0003.png',\n",
       " 'Indian/m.0b6044/m.0b6044_0006.png',\n",
       " 'African/m.03t_38/m.03t_38_0001.png',\n",
       " 'Asian/m.050hcd/m.050hcd_0003.png',\n",
       " 'African/m.0f_sw_/m.0f_sw__0004.png',\n",
       " 'African/m.0331f6/m.0331f6_0001.png',\n",
       " 'Indian/m.025w4wm/m.025w4wm_0005.png',\n",
       " 'Indian/m.03hmqrh/m.03hmqrh_0001.png',\n",
       " 'African/m.01tz35f/m.01tz35f_0003.png',\n",
       " 'African/m.07l2fk/m.07l2fk_0002.png',\n",
       " 'Asian/m.05zwzx/m.05zwzx_0002.png',\n",
       " 'Indian/m.0f21kx/m.0f21kx_0001.png',\n",
       " 'Caucasian/m.02x3d98/m.02x3d98_0001.png',\n",
       " 'Asian/m.026lr8/m.026lr8_0003.png',\n",
       " 'Asian/m.02l4f4/m.02l4f4_0003.png',\n",
       " 'Asian/m.08p7_l/m.08p7_l_0001.png',\n",
       " 'African/m.0h7y6x/m.0h7y6x_0001.png',\n",
       " 'Caucasian/m.0pf6v/m.0pf6v_0004.png',\n",
       " 'Caucasian/m.06_6j3/m.06_6j3_0002.png',\n",
       " 'Indian/m.0k28ny5/m.0k28ny5_0002.png',\n",
       " 'Caucasian/m.0g2mf1/m.0g2mf1_0003.png',\n",
       " 'African/m.090fpx/m.090fpx_0003.png',\n",
       " 'Indian/m.026nlpv/m.026nlpv_0003.png',\n",
       " 'African/m.05x7j28/m.05x7j28_0002.png',\n",
       " 'Indian/m.02x9x4q/m.02x9x4q_0002.png',\n",
       " 'Caucasian/m.03f0m74/m.03f0m74_0002.png',\n",
       " 'Indian/m.051jxq/m.051jxq_0001.png',\n",
       " 'African/m.03c7yx5/m.03c7yx5_0001.png',\n",
       " 'Indian/m.026m_gk/m.026m_gk_0003.png',\n",
       " 'Asian/m.07bp3v/m.07bp3v_0004.png',\n",
       " 'African/m.05zwryn/m.05zwryn_0002.png',\n",
       " 'Asian/m.03d_03/m.03d_03_0002.png',\n",
       " 'Caucasian/m.01ttk9t/m.01ttk9t_0004.png',\n",
       " 'Asian/m.02qv82y/m.02qv82y_0002.png',\n",
       " 'Asian/m.023570/m.023570_0002.png',\n",
       " 'Indian/m.09v3prf/m.09v3prf_0001.png',\n",
       " 'African/m.01kx43w/m.01kx43w_0003.png',\n",
       " 'Indian/m.0k0q8q/m.0k0q8q_0002.png',\n",
       " 'African/m.04m4gr/m.04m4gr_0003.png',\n",
       " 'Asian/m.02xfv6/m.02xfv6_0001.png',\n",
       " 'African/m.03cvfkn/m.03cvfkn_0001.png',\n",
       " 'Asian/m.0169x_/m.0169x__0001.png',\n",
       " 'Indian/m.02q38w4/m.02q38w4_0001.png',\n",
       " 'Indian/m.0cr6dzd/m.0cr6dzd_0004.png',\n",
       " 'African/m.0dsp1f/m.0dsp1f_0004.png',\n",
       " 'Caucasian/m.0268b9l/m.0268b9l_0003.png',\n",
       " 'Indian/m.04jnz1b/m.04jnz1b_0003.png',\n",
       " 'Asian/m.03c_6m2/m.03c_6m2_0003.png',\n",
       " 'Asian/m.0d5qwl/m.0d5qwl_0002.png',\n",
       " 'Asian/m.0gc1hm5/m.0gc1hm5_0003.png',\n",
       " 'Caucasian/m.0bbw9s9/m.0bbw9s9_0002.png',\n",
       " 'Asian/m.023dc4/m.023dc4_0004.png',\n",
       " 'African/m.07x9vf/m.07x9vf_0002.png',\n",
       " 'African/m.06w29d6/m.06w29d6_0001.png',\n",
       " 'Caucasian/m.0521wcd/m.0521wcd_0002.png',\n",
       " 'Indian/m.0cb5vm/m.0cb5vm_0003.png',\n",
       " 'Caucasian/m.03xv7z/m.03xv7z_0004.png',\n",
       " 'Indian/m.04n24yj/m.04n24yj_0003.png',\n",
       " 'African/m.02w2g_p/m.02w2g_p_0001.png',\n",
       " 'Asian/m.03byz_m/m.03byz_m_0001.png',\n",
       " 'Indian/m.0gpqf5/m.0gpqf5_0002.png',\n",
       " 'Indian/m.0cmn9c/m.0cmn9c_0003.png',\n",
       " 'Indian/m.0j4c8gg/m.0j4c8gg_0002.png',\n",
       " 'Caucasian/m.096589/m.096589_0002.png',\n",
       " 'Indian/m.0fd4_g/m.0fd4_g_0003.png',\n",
       " 'African/m.061bn9/m.061bn9_0001.png',\n",
       " 'African/m.026l038/m.026l038_0001.png',\n",
       " 'Asian/m.01nsmz/m.01nsmz_0001.png',\n",
       " 'Caucasian/m.076xd2d/m.076xd2d_0004.png',\n",
       " 'African/m.01m3hxr/m.01m3hxr_0001.png',\n",
       " 'Caucasian/m.02pqjp/m.02pqjp_0003.png',\n",
       " 'Caucasian/m.03czp1v/m.03czp1v_0002.png',\n",
       " 'Caucasian/m.020fm5/m.020fm5_0002.png',\n",
       " 'African/m.0h3vjbl/m.0h3vjbl_0002.png',\n",
       " 'Indian/m.09v3c8s/m.09v3c8s_0004.png',\n",
       " 'Caucasian/m.07vw7z/m.07vw7z_0003.png',\n",
       " 'African/m.0gl07xv/m.0gl07xv_0002.png',\n",
       " 'Asian/m.0cp_01/m.0cp_01_0002.png',\n",
       " 'Indian/m.0h4pss/m.0h4pss_0003.png',\n",
       " 'Indian/m.04gv9_5/m.04gv9_5_0001.png',\n",
       " 'African/m.01xdx6/m.01xdx6_0002.png',\n",
       " 'Asian/m.05lrx4/m.05lrx4_0002.png',\n",
       " 'African/m.04f4zf7/m.04f4zf7_0001.png',\n",
       " 'Indian/m.02w3085/m.02w3085_0004.png',\n",
       " 'African/m.0dvhrz/m.0dvhrz_0003.png',\n",
       " 'Asian/m.04mndzj/m.04mndzj_0002.png',\n",
       " 'African/m.06zqsws/m.06zqsws_0002.png',\n",
       " 'Indian/m.02g_65/m.02g_65_0002.png',\n",
       " 'African/m.0p786p4/m.0p786p4_0005.png',\n",
       " 'Indian/m.0b8zqd/m.0b8zqd_0001.png',\n",
       " 'Caucasian/m.06w2rl/m.06w2rl_0001.png',\n",
       " 'Asian/m.01yzlj/m.01yzlj_0001.png',\n",
       " 'Asian/m.030mlg/m.030mlg_0001.png',\n",
       " 'Caucasian/m.02qck3r/m.02qck3r_0003.png',\n",
       " 'Asian/m.0cdnv_/m.0cdnv__0005.png',\n",
       " 'African/m.0h_5m6x/m.0h_5m6x_0001.png',\n",
       " 'African/m.08dt_q/m.08dt_q_0003.png',\n",
       " 'Indian/m.0dr0lf/m.0dr0lf_0003.png',\n",
       " 'Asian/m.0n_9frn/m.0n_9frn_0002.png',\n",
       " 'Asian/m.04zvqgd/m.04zvqgd_0005.png',\n",
       " 'African/m.04v8_n/m.04v8_n_0002.png',\n",
       " 'African/m.03mdl8/m.03mdl8_0003.png',\n",
       " 'Indian/m.0bmf76t/m.0bmf76t_0002.png',\n",
       " 'Indian/m.0bs787w/m.0bs787w_0001.png',\n",
       " 'African/m.0c7t5z/m.0c7t5z_0001.png',\n",
       " 'Caucasian/m.05zrn1w/m.05zrn1w_0003.png',\n",
       " 'Indian/m.025zr2r/m.025zr2r_0001.png',\n",
       " 'Indian/m.03h4gg9/m.03h4gg9_0003.png',\n",
       " 'Caucasian/m.070ycn/m.070ycn_0001.png',\n",
       " 'Asian/m.0g56y6t/m.0g56y6t_0003.png',\n",
       " 'African/m.0gfhg0w/m.0gfhg0w_0002.png',\n",
       " 'Caucasian/m.047rs3/m.047rs3_0002.png',\n",
       " 'Asian/m.0lq5svr/m.0lq5svr_0004.png',\n",
       " 'Caucasian/m.07xhz6/m.07xhz6_0001.png',\n",
       " 'Caucasian/m.03c2x_h/m.03c2x_h_0001.png',\n",
       " 'Caucasian/m.05c4nz/m.05c4nz_0001.png',\n",
       " 'Indian/m.0c7nkn/m.0c7nkn_0003.png',\n",
       " 'Caucasian/m.07c9z5/m.07c9z5_0004.png',\n",
       " 'African/m.01j4p1/m.01j4p1_0003.png',\n",
       " 'Indian/m.0k2lz84/m.0k2lz84_0003.png',\n",
       " 'Caucasian/m.08prq0/m.08prq0_0003.png',\n",
       " 'Caucasian/m.03kcyd/m.03kcyd_0004.png',\n",
       " 'Asian/m.055gm00/m.055gm00_0001.png',\n",
       " 'Indian/m.01ryns0/m.01ryns0_0001.png',\n",
       " 'African/m.087qnf/m.087qnf_0002.png',\n",
       " 'Indian/m.01w7n7b/m.01w7n7b_0002.png',\n",
       " 'Asian/m.01syzdc/m.01syzdc_0005.png',\n",
       " 'Indian/m.03mdk5/m.03mdk5_0003.png',\n",
       " 'Indian/m.0sgg_cr/m.0sgg_cr_0003.png',\n",
       " 'African/m.07r523/m.07r523_0003.png',\n",
       " 'Caucasian/m.0jwr6z9/m.0jwr6z9_0002.png',\n",
       " 'African/m.0gfh4tc/m.0gfh4tc_0001.png',\n",
       " 'Caucasian/m.026tgty/m.026tgty_0002.png',\n",
       " 'Asian/m.08z2gy/m.08z2gy_0002.png',\n",
       " 'Indian/m.0b6h1nn/m.0b6h1nn_0001.png',\n",
       " 'Asian/m.09gb8cm/m.09gb8cm_0003.png',\n",
       " 'Asian/m.02w_xk/m.02w_xk_0003.png',\n",
       " 'Asian/m.06yrwt/m.06yrwt_0006.png',\n",
       " 'African/m.03ykfwc/m.03ykfwc_0004.png',\n",
       " 'Indian/m.06t5x_/m.06t5x__0001.png',\n",
       " 'Asian/m.048xxl/m.048xxl_0005.png',\n",
       " 'Asian/m.03c_6m2/m.03c_6m2_0001.png',\n",
       " 'Indian/m.02qj53l/m.02qj53l_0002.png',\n",
       " 'Asian/m.01g3fb/m.01g3fb_0003.png',\n",
       " 'African/m.051c19/m.051c19_0003.png',\n",
       " 'African/m.03pcpj/m.03pcpj_0001.png',\n",
       " 'African/m.0k2g2nc/m.0k2g2nc_0003.png',\n",
       " 'Indian/m.03ct0gd/m.03ct0gd_0002.png',\n",
       " 'African/m.07drlm/m.07drlm_0001.png',\n",
       " 'Indian/m.04myvmy/m.04myvmy_0003.png',\n",
       " 'African/m.0b26qx/m.0b26qx_0003.png',\n",
       " 'Asian/m.05h2141/m.05h2141_0003.png',\n",
       " 'Caucasian/m.08rswk/m.08rswk_0004.png',\n",
       " 'Asian/m.05ltb8/m.05ltb8_0001.png',\n",
       " 'Asian/m.03ptt2/m.03ptt2_0002.png',\n",
       " 'Caucasian/m.0kk4q/m.0kk4q_0002.png',\n",
       " 'Asian/m.0g55mhy/m.0g55mhy_0004.png',\n",
       " 'African/m.0k3npf0/m.0k3npf0_0003.png',\n",
       " 'Caucasian/m.026_cgm/m.026_cgm_0004.png',\n",
       " 'Asian/m.0dfclw/m.0dfclw_0004.png',\n",
       " 'African/m.01w6m_0/m.01w6m_0_0001.png',\n",
       " 'Caucasian/m.039xxb/m.039xxb_0005.png',\n",
       " 'Caucasian/m.01vm3rs/m.01vm3rs_0004.png',\n",
       " 'African/m.0cwnbz/m.0cwnbz_0001.png',\n",
       " 'Asian/m.0h7qpfc/m.0h7qpfc_0003.png',\n",
       " 'Caucasian/m.06wx5c/m.06wx5c_0002.png',\n",
       " 'Indian/m.05p7mlq/m.05p7mlq_0002.png',\n",
       " 'Caucasian/m.07k9j4/m.07k9j4_0003.png',\n",
       " 'African/m.0d761n/m.0d761n_0001.png',\n",
       " 'African/m.0h_f7cs/m.0h_f7cs_0001.png',\n",
       " 'African/m.02w_f6b/m.02w_f6b_0003.png',\n",
       " 'Caucasian/m.01kgh3/m.01kgh3_0002.png',\n",
       " 'Indian/m.0j8dp8d/m.0j8dp8d_0003.png',\n",
       " 'Caucasian/m.01h_3z/m.01h_3z_0002.png',\n",
       " 'Caucasian/m.070_kf/m.070_kf_0003.png',\n",
       " 'African/m.051jrr/m.051jrr_0004.png',\n",
       " 'Indian/m.027q5vk/m.027q5vk_0004.png',\n",
       " 'Asian/m.0gffbmh/m.0gffbmh_0003.png',\n",
       " 'African/m.042t32/m.042t32_0001.png',\n",
       " 'Indian/m.05mzg5y/m.05mzg5y_0002.png',\n",
       " 'Indian/m.06jdb3/m.06jdb3_0001.png',\n",
       " 'African/m.01t14w/m.01t14w_0001.png',\n",
       " 'African/m.0g3kch/m.0g3kch_0003.png',\n",
       " 'Asian/m.03cj7mx/m.03cj7mx_0005.png',\n",
       " 'African/m.05zw_y6/m.05zw_y6_0004.png',\n",
       " 'Caucasian/m.0b_c03/m.0b_c03_0003.png',\n",
       " 'Caucasian/m.03hh_d_/m.03hh_d__0004.png',\n",
       " 'African/m.0gfd_j2/m.0gfd_j2_0002.png',\n",
       " 'African/m.04nt5w/m.04nt5w_0003.png',\n",
       " 'Indian/m.0gfg4h9/m.0gfg4h9_0002.png',\n",
       " 'Asian/m.02px67h/m.02px67h_0002.png',\n",
       " 'Caucasian/m.04cvpmr/m.04cvpmr_0004.png',\n",
       " 'Caucasian/m.0b0j6v/m.0b0j6v_0003.png',\n",
       " 'Caucasian/m.073djs/m.073djs_0002.png',\n",
       " 'Indian/m.096q_f/m.096q_f_0003.png',\n",
       " 'Indian/m.0j_5xy/m.0j_5xy_0003.png',\n",
       " 'African/m.023915/m.023915_0001.png',\n",
       " 'Asian/m.0czd0f0/m.0czd0f0_0003.png',\n",
       " 'Indian/m.027vy8b/m.027vy8b_0001.png',\n",
       " 'Asian/m.020749/m.020749_0001.png',\n",
       " 'Asian/m.087d4m/m.087d4m_0003.png',\n",
       " 'Indian/m.0gtwr8m/m.0gtwr8m_0001.png',\n",
       " 'Indian/m.04f5d4c/m.04f5d4c_0003.png',\n",
       " 'Asian/m.051hfg/m.051hfg_0007.png',\n",
       " 'Indian/m.0fq0cg7/m.0fq0cg7_0002.png',\n",
       " 'African/m.0cw2wc/m.0cw2wc_0002.png',\n",
       " 'Caucasian/m.0j66tfg/m.0j66tfg_0004.png',\n",
       " 'Caucasian/m.09csnn/m.09csnn_0002.png',\n",
       " 'Asian/m.06b6t_/m.06b6t__0003.png',\n",
       " 'Asian/m.04ym_v/m.04ym_v_0002.png',\n",
       " 'Caucasian/m.05mgqb/m.05mgqb_0001.png',\n",
       " 'Caucasian/m.08_nsh/m.08_nsh_0001.png',\n",
       " 'Indian/m.04dwwp/m.04dwwp_0003.png',\n",
       " 'Indian/m.041pg0/m.041pg0_0003.png',\n",
       " 'African/m.01t4wm/m.01t4wm_0003.png',\n",
       " 'African/m.05p823/m.05p823_0001.png',\n",
       " 'Caucasian/m.07ldj3/m.07ldj3_0004.png',\n",
       " 'Indian/m.02q11bs/m.02q11bs_0003.png',\n",
       " 'Indian/m.048srr/m.048srr_0002.png',\n",
       " 'African/m.02vst7p/m.02vst7p_0002.png',\n",
       " 'Caucasian/m.04bnvr/m.04bnvr_0002.png',\n",
       " 'Caucasian/m.0rfdr0b/m.0rfdr0b_0001.png',\n",
       " 'Indian/m.09vf3t/m.09vf3t_0001.png',\n",
       " 'Indian/m.0crdf03/m.0crdf03_0001.png',\n",
       " 'Indian/m.08g18f/m.08g18f_0002.png',\n",
       " 'Indian/m.027gqlw/m.027gqlw_0002.png',\n",
       " 'Caucasian/m.04wr44/m.04wr44_0003.png',\n",
       " 'Indian/m.0417bs9/m.0417bs9_0002.png',\n",
       " 'Indian/m.01q8vtc/m.01q8vtc_0003.png',\n",
       " 'Asian/m.02b6km/m.02b6km_0001.png',\n",
       " 'Caucasian/m.0dlk32/m.0dlk32_0001.png',\n",
       " 'Asian/m.09h3b6/m.09h3b6_0006.png',\n",
       " 'Asian/m.06224x/m.06224x_0003.png',\n",
       " 'Caucasian/m.02fr47/m.02fr47_0002.png',\n",
       " 'Asian/m.03d_03/m.03d_03_0001.png',\n",
       " 'Indian/m.020s9f/m.020s9f_0002.png',\n",
       " 'African/m.06l2_/m.06l2__0002.png',\n",
       " 'Asian/m.07p9_w/m.07p9_w_0002.png',\n",
       " 'Asian/m.06c1fd/m.06c1fd_0001.png',\n",
       " 'Caucasian/m.04hh0d/m.04hh0d_0002.png',\n",
       " 'Asian/m.0265gps/m.0265gps_0002.png',\n",
       " 'Indian/m.01j6jv/m.01j6jv_0003.png',\n",
       " 'Asian/m.02gtmx/m.02gtmx_0006.png',\n",
       " 'Indian/m.046zpr/m.046zpr_0002.png',\n",
       " 'African/m.0cnzbrt/m.0cnzbrt_0001.png',\n",
       " 'African/m.0fw3k1/m.0fw3k1_0002.png',\n",
       " 'African/m.0450ck/m.0450ck_0002.png',\n",
       " 'African/m.03n6sj/m.03n6sj_0002.png',\n",
       " 'Caucasian/m.07tm2y/m.07tm2y_0001.png',\n",
       " 'Indian/m.03qhpf2/m.03qhpf2_0001.png',\n",
       " 'Indian/m.0ks45n/m.0ks45n_0002.png',\n",
       " 'Caucasian/m.03bptr/m.03bptr_0001.png',\n",
       " 'Asian/m.0kcgvy5/m.0kcgvy5_0001.png',\n",
       " 'Caucasian/m.03d9sf/m.03d9sf_0001.png',\n",
       " 'African/m.03cqw7m/m.03cqw7m_0005.png',\n",
       " 'African/m.05b_6z2/m.05b_6z2_0003.png',\n",
       " 'Indian/m.04gln4x/m.04gln4x_0003.png',\n",
       " 'Asian/m.076zpfm/m.076zpfm_0004.png',\n",
       " 'African/m.0r8x_gx/m.0r8x_gx_0003.png',\n",
       " 'African/m.0jl04kc/m.0jl04kc_0001.png',\n",
       " 'Indian/m.047nkxw/m.047nkxw_0003.png',\n",
       " 'African/m.03v8j3/m.03v8j3_0003.png',\n",
       " 'Asian/m.02jzjr/m.02jzjr_0004.png',\n",
       " 'Caucasian/m.02r2043/m.02r2043_0001.png',\n",
       " 'African/m.01lmtl/m.01lmtl_0001.png',\n",
       " 'Caucasian/m.05sy8/m.05sy8_0003.png',\n",
       " 'Indian/m.0bht62/m.0bht62_0001.png',\n",
       " 'African/m.09g7pbs/m.09g7pbs_0002.png',\n",
       " 'Asian/m.02rx4bw/m.02rx4bw_0002.png',\n",
       " 'Asian/m.0c419y/m.0c419y_0001.png',\n",
       " 'African/m.0795dz/m.0795dz_0001.png',\n",
       " 'African/m.05r6qt/m.05r6qt_0003.png',\n",
       " 'African/m.03yzzq/m.03yzzq_0002.png',\n",
       " 'Asian/m.0411c7c/m.0411c7c_0002.png',\n",
       " 'Asian/m.027270l/m.027270l_0003.png',\n",
       " 'Indian/m.059md3/m.059md3_0002.png',\n",
       " 'African/m.04l4q6/m.04l4q6_0003.png',\n",
       " 'Caucasian/m.01wm1z7/m.01wm1z7_0003.png',\n",
       " 'Asian/m.03cx2x/m.03cx2x_0007.png',\n",
       " 'African/m.08j85q/m.08j85q_0004.png',\n",
       " 'Asian/m.0fc2qq/m.0fc2qq_0003.png',\n",
       " 'Caucasian/m.0h3tgnw/m.0h3tgnw_0002.png',\n",
       " 'Asian/m.06yj75/m.06yj75_0003.png',\n",
       " 'Asian/m.067q5t/m.067q5t_0002.png',\n",
       " 'Indian/m.0h3x88y/m.0h3x88y_0001.png',\n",
       " 'Asian/m.0ndsl6m/m.0ndsl6m_0003.png',\n",
       " 'Indian/m.0280xl6/m.0280xl6_0001.png',\n",
       " 'Asian/m.0nhhrrz/m.0nhhrrz_0005.png',\n",
       " 'Indian/m.0br_twh/m.0br_twh_0003.png',\n",
       " 'Asian/m.02890bv/m.02890bv_0001.png',\n",
       " 'Indian/m.01pgkq6/m.01pgkq6_0003.png',\n",
       " 'Asian/m.024sr9/m.024sr9_0001.png',\n",
       " 'Caucasian/m.027hr9k/m.027hr9k_0002.png',\n",
       " 'Caucasian/m.02r9sfp/m.02r9sfp_0001.png',\n",
       " 'Caucasian/m.027j8ws/m.027j8ws_0004.png',\n",
       " 'Asian/m.01tkk5c/m.01tkk5c_0005.png',\n",
       " 'Indian/m.0glnnsy/m.0glnnsy_0001.png',\n",
       " 'African/m.05cy47/m.05cy47_0001.png',\n",
       " 'Indian/m.026p3wd/m.026p3wd_0003.png',\n",
       " 'Asian/m.0ksttk/m.0ksttk_0001.png',\n",
       " 'Caucasian/m.05gzxg/m.05gzxg_0003.png',\n",
       " 'African/m.07drlm/m.07drlm_0003.png',\n",
       " 'Indian/m.0bccg_/m.0bccg__0002.png',\n",
       " 'Indian/m.0ds5_8_/m.0ds5_8__0004.png',\n",
       " 'Caucasian/m.01ndxh/m.01ndxh_0001.png',\n",
       " 'African/m.04v_0f/m.04v_0f_0003.png',\n",
       " 'Caucasian/m.03w6yx/m.03w6yx_0002.png',\n",
       " 'Caucasian/m.03r0b_/m.03r0b__0001.png',\n",
       " 'African/m.02jhrg/m.02jhrg_0001.png',\n",
       " 'Caucasian/m.041xfx/m.041xfx_0003.png',\n",
       " 'Caucasian/m.046mnc/m.046mnc_0003.png',\n",
       " 'Caucasian/m.0cjyjg/m.0cjyjg_0001.png',\n",
       " 'African/m.08fz3f/m.08fz3f_0001.png',\n",
       " 'Caucasian/m.01hf4p/m.01hf4p_0004.png',\n",
       " 'Indian/m.02853fy/m.02853fy_0003.png',\n",
       " 'Caucasian/m.0hnc76t/m.0hnc76t_0001.png',\n",
       " 'Caucasian/m.01r5wc/m.01r5wc_0003.png',\n",
       " 'African/m.032nf0/m.032nf0_0002.png',\n",
       " 'Caucasian/m.05nz3b/m.05nz3b_0001.png',\n",
       " 'African/m.09mmh4/m.09mmh4_0001.png',\n",
       " 'Indian/m.05c2nlk/m.05c2nlk_0004.png',\n",
       " 'Asian/m.02857j2/m.02857j2_0004.png',\n",
       " 'Caucasian/m.03_61b/m.03_61b_0003.png',\n",
       " 'Indian/m.027nz7r/m.027nz7r_0003.png',\n",
       " 'African/m.03xp8dq/m.03xp8dq_0004.png',\n",
       " 'African/m.0gyts84/m.0gyts84_0003.png',\n",
       " 'Asian/m.056q2j/m.056q2j_0002.png',\n",
       " 'African/m.03f1y6p/m.03f1y6p_0002.png',\n",
       " 'Caucasian/m.067hq2/m.067hq2_0001.png',\n",
       " 'Asian/m.027xl2v/m.027xl2v_0004.png',\n",
       " 'Indian/m.023sng/m.023sng_0002.png',\n",
       " 'Asian/m.03rl7s/m.03rl7s_0001.png',\n",
       " 'Indian/m.03fw4k/m.03fw4k_0003.png',\n",
       " 'Asian/m.025hpm/m.025hpm_0002.png',\n",
       " 'Asian/m.0g55mhy/m.0g55mhy_0002.png',\n",
       " 'African/m.04v858/m.04v858_0003.png',\n",
       " 'Caucasian/m.0cl29w/m.0cl29w_0003.png',\n",
       " 'Asian/m.023j9v/m.023j9v_0002.png',\n",
       " 'African/m.01vz63g/m.01vz63g_0004.png',\n",
       " 'African/m.012gl8/m.012gl8_0003.png',\n",
       " 'Indian/m.0gg6jt2/m.0gg6jt2_0001.png',\n",
       " 'Indian/m.0gfd4fq/m.0gfd4fq_0002.png',\n",
       " 'African/m.06xqdy/m.06xqdy_0002.png',\n",
       " 'Caucasian/m.0ds9hhq/m.0ds9hhq_0001.png',\n",
       " 'Asian/m.031513/m.031513_0002.png',\n",
       " 'Indian/m.04y8bb7/m.04y8bb7_0002.png',\n",
       " 'Indian/m.0g5nr1/m.0g5nr1_0002.png',\n",
       " 'Caucasian/m.05_x0/m.05_x0_0001.png',\n",
       " 'Asian/m.0gc1hm5/m.0gc1hm5_0002.png',\n",
       " 'Caucasian/m.07zmc2/m.07zmc2_0003.png',\n",
       " 'Indian/m.028hpk/m.028hpk_0003.png',\n",
       " 'Asian/m.0j3fjt7/m.0j3fjt7_0001.png',\n",
       " 'Caucasian/m.09z6mc/m.09z6mc_0002.png',\n",
       " 'Asian/m.02r7lyb/m.02r7lyb_0001.png',\n",
       " 'Indian/m.05t0fwn/m.05t0fwn_0005.png',\n",
       " 'Caucasian/m.03tg3x/m.03tg3x_0004.png',\n",
       " 'Indian/m.026m_m0/m.026m_m0_0002.png',\n",
       " 'Caucasian/m.027klgs/m.027klgs_0004.png',\n",
       " 'Asian/m.039p3p/m.039p3p_0004.png',\n",
       " 'African/m.030m41/m.030m41_0001.png',\n",
       " 'Indian/m.07ds0w/m.07ds0w_0003.png',\n",
       " 'Indian/m.028bs1p/m.028bs1p_0003.png',\n",
       " 'Indian/m.0h970l2/m.0h970l2_0002.png',\n",
       " 'Caucasian/m.01nf051/m.01nf051_0002.png',\n",
       " 'African/m.01m89jz/m.01m89jz_0003.png',\n",
       " 'Indian/m.0dsffkf/m.0dsffkf_0004.png',\n",
       " 'Indian/m.01wt3z/m.01wt3z_0004.png',\n",
       " 'Indian/m.03y7r0z/m.03y7r0z_0002.png',\n",
       " 'Caucasian/m.0412chs/m.0412chs_0001.png',\n",
       " 'Caucasian/m.01wbzhv/m.01wbzhv_0004.png',\n",
       " 'Asian/m.0gc69ms/m.0gc69ms_0004.png',\n",
       " 'Asian/m.01rf6f/m.01rf6f_0003.png',\n",
       " 'African/m.098d2c1/m.098d2c1_0002.png',\n",
       " 'African/m.0d5d_v/m.0d5d_v_0003.png',\n",
       " 'Indian/m.03d09qd/m.03d09qd_0003.png',\n",
       " 'Caucasian/m.027q3qg/m.027q3qg_0002.png',\n",
       " 'African/m.034020/m.034020_0003.png',\n",
       " 'Indian/m.026r159/m.026r159_0004.png',\n",
       " 'Caucasian/m.027__w1/m.027__w1_0002.png',\n",
       " 'African/m.027tssp/m.027tssp_0001.png',\n",
       " 'Caucasian/m.04vwzj3/m.04vwzj3_0002.png',\n",
       " 'Asian/m.05gk6d/m.05gk6d_0001.png',\n",
       " 'Indian/m.026sl3m/m.026sl3m_0001.png',\n",
       " 'Asian/m.0lz2n/m.0lz2n_0001.png',\n",
       " 'Indian/m.04gfkr/m.04gfkr_0003.png',\n",
       " 'African/m.022w90/m.022w90_0001.png',\n",
       " 'African/m.01ws2cd/m.01ws2cd_0002.png',\n",
       " 'African/m.07qnz0/m.07qnz0_0002.png',\n",
       " 'Caucasian/m.02nlps/m.02nlps_0002.png',\n",
       " 'Asian/m.07v6k9/m.07v6k9_0001.png',\n",
       " 'African/m.02rh7ds/m.02rh7ds_0004.png',\n",
       " 'Asian/m.02x2__s/m.02x2__s_0003.png',\n",
       " 'African/m.056d7x/m.056d7x_0001.png',\n",
       " 'Caucasian/m.0pl0f4t/m.0pl0f4t_0003.png',\n",
       " 'Caucasian/m.0g5r2wh/m.0g5r2wh_0001.png',\n",
       " 'African/m.03hzpnk/m.03hzpnk_0004.png',\n",
       " 'Indian/m.086zzh/m.086zzh_0004.png',\n",
       " 'African/m.0ghglf/m.0ghglf_0003.png',\n",
       " 'Indian/m.0gz7gh/m.0gz7gh_0005.png',\n",
       " 'Asian/m.0j63nd5/m.0j63nd5_0004.png',\n",
       " 'Asian/m.09k6wk0/m.09k6wk0_0002.png',\n",
       " 'Caucasian/m.01tyq5/m.01tyq5_0002.png',\n",
       " 'Caucasian/m.0drwqtn/m.0drwqtn_0003.png',\n",
       " 'Caucasian/m.04mx6tt/m.04mx6tt_0003.png',\n",
       " 'Indian/m.02w_8zj/m.02w_8zj_0004.png',\n",
       " 'African/m.08xxx9/m.08xxx9_0001.png',\n",
       " 'African/m.05p5171/m.05p5171_0001.png',\n",
       " 'Asian/m.05jxt_/m.05jxt__0003.png',\n",
       " 'African/m.04ggs_/m.04ggs__0004.png',\n",
       " 'Indian/m.05vvl1/m.05vvl1_0003.png',\n",
       " 'Asian/m.01vrw27/m.01vrw27_0001.png',\n",
       " 'Caucasian/m.06yg_v/m.06yg_v_0003.png',\n",
       " 'Asian/m.0sgqn5d/m.0sgqn5d_0004.png',\n",
       " 'Indian/m.0dlkm7l/m.0dlkm7l_0004.png',\n",
       " 'Caucasian/m.0bpgsl/m.0bpgsl_0001.png',\n",
       " 'African/m.0r8dzb3/m.0r8dzb3_0001.png',\n",
       " 'Asian/m.036572/m.036572_0001.png',\n",
       " 'African/m.01q_jfp/m.01q_jfp_0001.png',\n",
       " 'Caucasian/m.03xv7z/m.03xv7z_0002.png',\n",
       " 'Indian/m.02n5b3/m.02n5b3_0001.png',\n",
       " 'Caucasian/m.03kw3g/m.03kw3g_0004.png',\n",
       " 'African/m.0b7bj96/m.0b7bj96_0003.png',\n",
       " 'Asian/m.0j63pr1/m.0j63pr1_0002.png',\n",
       " 'African/m.07r46t/m.07r46t_0003.png',\n",
       " 'Asian/m.02dvtt/m.02dvtt_0002.png',\n",
       " 'Caucasian/m.0vp0s6x/m.0vp0s6x_0001.png',\n",
       " 'Indian/m.07d4hh/m.07d4hh_0002.png',\n",
       " 'Indian/m.04gt1b5/m.04gt1b5_0003.png',\n",
       " 'Indian/m.0fr7nt/m.0fr7nt_0002.png',\n",
       " 'Indian/m.0g5b2d5/m.0g5b2d5_0003.png',\n",
       " 'Indian/m.08nv6s/m.08nv6s_0002.png',\n",
       " 'Asian/m.03f5mf/m.03f5mf_0002.png',\n",
       " 'Caucasian/m.0b5jqz/m.0b5jqz_0001.png',\n",
       " 'African/m.0273fxl/m.0273fxl_0004.png',\n",
       " 'Indian/m.09g6ck4/m.09g6ck4_0001.png',\n",
       " 'Caucasian/m.0ch929y/m.0ch929y_0001.png',\n",
       " 'Caucasian/m.07zjy5/m.07zjy5_0002.png',\n",
       " 'Indian/m.07jxhn/m.07jxhn_0003.png',\n",
       " 'Caucasian/m.0cg2l6/m.0cg2l6_0003.png',\n",
       " 'African/m.0fpgg4/m.0fpgg4_0005.png',\n",
       " 'Indian/m.03nwmpd/m.03nwmpd_0001.png',\n",
       " 'African/m.09t4z5/m.09t4z5_0003.png',\n",
       " 'African/m.0bx_t3s/m.0bx_t3s_0001.png',\n",
       " 'African/m.0bs8wn2/m.0bs8wn2_0002.png',\n",
       " 'Indian/m.01q8x1z/m.01q8x1z_0003.png',\n",
       " 'African/m.0czbtdm/m.0czbtdm_0003.png',\n",
       " 'African/m.09g82r/m.09g82r_0003.png',\n",
       " 'Indian/m.06zqqv/m.06zqqv_0004.png',\n",
       " 'Indian/m.04rzsf/m.04rzsf_0003.png',\n",
       " 'African/m.03nqkk4/m.03nqkk4_0002.png',\n",
       " 'African/m.098jhf/m.098jhf_0003.png',\n",
       " 'Caucasian/m.0gx01vq/m.0gx01vq_0003.png',\n",
       " 'Caucasian/m.07tm2y/m.07tm2y_0003.png',\n",
       " 'Caucasian/m.08324h/m.08324h_0001.png',\n",
       " 'Indian/m.031p03/m.031p03_0002.png',\n",
       " 'Asian/m.0cnx_py/m.0cnx_py_0003.png',\n",
       " 'Indian/m.02qtcq/m.02qtcq_0004.png',\n",
       " 'Indian/m.07s3_p2/m.07s3_p2_0001.png',\n",
       " 'Caucasian/m.01ljk0w/m.01ljk0w_0003.png',\n",
       " 'Indian/m.02vvvf5/m.02vvvf5_0002.png',\n",
       " 'Caucasian/m.08h_2n/m.08h_2n_0002.png',\n",
       " 'Indian/m.09tf4pl/m.09tf4pl_0002.png',\n",
       " 'Asian/m.05m_l9/m.05m_l9_0002.png',\n",
       " 'Asian/m.0gtqcdp/m.0gtqcdp_0004.png',\n",
       " 'African/m.0x1lfrd/m.0x1lfrd_0001.png',\n",
       " 'Indian/m.0hzpcpc/m.0hzpcpc_0003.png',\n",
       " 'African/m.01pl8b6/m.01pl8b6_0001.png',\n",
       " 'Indian/m.03cn6js/m.03cn6js_0006.png',\n",
       " 'Asian/m.036sd4/m.036sd4_0002.png',\n",
       " 'Asian/m.052_wzm/m.052_wzm_0006.png',\n",
       " 'Asian/m.0gkl7_d/m.0gkl7_d_0001.png',\n",
       " 'African/m.076yq88/m.076yq88_0001.png',\n",
       " 'Caucasian/m.05h55p7/m.05h55p7_0003.png',\n",
       " 'Caucasian/m.0ghybz/m.0ghybz_0002.png',\n",
       " 'Indian/m.04rs03/m.04rs03_0001.png',\n",
       " 'African/m.0h3mv1d/m.0h3mv1d_0004.png',\n",
       " 'Caucasian/m.046x73/m.046x73_0002.png',\n",
       " 'Indian/m.06vsvl/m.06vsvl_0003.png',\n",
       " 'African/m.02q26w2/m.02q26w2_0001.png',\n",
       " 'Asian/m.0bv889y/m.0bv889y_0002.png',\n",
       " 'Indian/m.05m_md/m.05m_md_0003.png',\n",
       " 'Asian/m.02z1pvy/m.02z1pvy_0002.png',\n",
       " 'Indian/m.03c3w2r/m.03c3w2r_0001.png',\n",
       " 'Caucasian/m.0gwyr8_/m.0gwyr8__0001.png',\n",
       " 'African/m.0bwl6h/m.0bwl6h_0004.png',\n",
       " 'African/m.08mxgv/m.08mxgv_0003.png',\n",
       " 'Asian/m.09l7h4/m.09l7h4_0005.png',\n",
       " 'African/m.064npqy/m.064npqy_0002.png',\n",
       " 'Indian/m.0gc2v9d/m.0gc2v9d_0001.png',\n",
       " 'African/m.01n1p7j/m.01n1p7j_0001.png',\n",
       " 'Asian/m.05p3snt/m.05p3snt_0002.png',\n",
       " 'Indian/m.0gg5sfr/m.0gg5sfr_0005.png',\n",
       " 'Caucasian/m.0k216/m.0k216_0002.png',\n",
       " 'Caucasian/m.030w0v/m.030w0v_0003.png',\n",
       " 'Caucasian/m.0cgr5j/m.0cgr5j_0003.png',\n",
       " 'Indian/m.08091sk/m.08091sk_0001.png',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ATTRIBUTE_INDECIES = {\n",
    "    'skin_type': 0,\n",
    "    'lip_type': 1,\n",
    "    'nose_type': 2,\n",
    "    'eye_type': 3,\n",
    "    'hair_type': 4,\n",
    "    'hair_color': 5\n",
    "}\n",
    "\n",
    "def save_race_based_predictions(\n",
    "        models,  \n",
    "        dataloader, \n",
    "        device, \n",
    "        prediction_save_dir,\n",
    "        attributes\n",
    "    ):\n",
    "    all_predictions = {'Indian': {attr: torch.tensor([]) for attr in attributes}, \n",
    "                       'Caucasian': {attr: torch.tensor([]) for attr in attributes}, \n",
    "                       'Asian': {attr: torch.tensor([]) for attr in attributes},  \n",
    "                       'African': {attr: torch.tensor([]) for attr in attributes}}\n",
    "    all_labels = {'Indian': {attr: torch.tensor([]) for attr in attributes}, \n",
    "                  'Caucasian': {attr: torch.tensor([]) for attr in attributes}, \n",
    "                  'Asian': {attr: torch.tensor([]) for attr in attributes}, \n",
    "                  'African': {attr: torch.tensor([]) for attr in attributes}}\n",
    "    all_file_names = {\n",
    "        'Indian': {attr: [] for attr in attributes}, \n",
    "        'Caucasian': {attr: [] for attr in attributes}, \n",
    "        'Asian': {attr: [] for attr in attributes},  \n",
    "        'African': {attr: [] for attr in attributes}\n",
    "    }\n",
    "    print(f'prediction_save_dir: {prediction_save_dir}')\n",
    "    dataloader = tqdm(dataloader, desc=\"Getting Predictions\", unit=\"batch\")\n",
    "    with torch.no_grad():\n",
    "        for j, model in enumerate(models):\n",
    "            model.eval()\n",
    "            for _, data in enumerate(dataloader):\n",
    "                inputs, labels, race, file_names = data\n",
    "                file_names = np.array(list(file_names))\n",
    "                race = np.array(race)\n",
    "\n",
    "                inputs = inputs.to(torch.float).to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                for i, (head, predictions) in enumerate(outputs.items()):\n",
    "                    head_preds = predictions.argmax(dim=1).cpu()\n",
    "\n",
    "                    for race_label in all_labels:\n",
    "                        race_indices = np.array((race == race_label).nonzero()[0])\n",
    "                        race_predictions = head_preds[race_indices]\n",
    "                        race_file_names = file_names[race_indices]\n",
    "                        race_labels = labels[:, ATTRIBUTE_INDECIES[head]][race_indices]\n",
    "                    \n",
    "                        all_predictions[race_label][head] = torch.cat((all_predictions[race_label][head], race_predictions.to('cpu')), dim=0)\n",
    "                        all_labels[race_label][head] = torch.cat((all_labels[race_label][head], race_labels.to('cpu')), dim=0)\n",
    "                        all_file_names[race_label][head].extend(list(race_file_names))\n",
    "    # with open(prediction_save_dir + '/sep_predictions.pkl', 'wb+') as f:\n",
    "    #     pickle.dump(all_predictions, f)\n",
    "    # with open(prediction_save_dir + '/sep_labels.pkl', 'wb+') as f:\n",
    "    #     pickle.dump(all_labels, f)\n",
    "\n",
    "\n",
    "    return all_predictions, all_labels, all_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheng2020-attn\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_0001\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 35.96batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_0009\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.71batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_1\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.56batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_2\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.52batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_3\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.60batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperprior\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_0001\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.61batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_0009\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.31batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_1\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.44batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_2\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.43batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_3\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.65batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mbt2018\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_0001\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 38.03batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_0009\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.61batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_1\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.66batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_2\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.53batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "q_3\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.49batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qarv\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "lmb_1\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.28batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "lmb_4\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.13batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "lmb_8\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.09batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "lmb_16\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.34batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "lmb_32\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.39batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qres17m\n",
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "1\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.78batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "3\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.86batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "6\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.77batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "9\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.60batch/s]\n",
      "/home/tianqiu/.conda/envs/torch2_2/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr: [[0 'm.011y5k_0003.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [1 'm.011y5k_0004.png' 'African/m.011y5k' ... 1 1 0]\n",
      " [2 'm.011y5k_0002.png' 'African/m.011y5k' ... 1 1 0]\n",
      " ...\n",
      " [40604 'm.0_4pw_0004.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40605 'm.0_4pw_0003.png' 'Indian/m.0_4pw' ... 1 2 0]\n",
      " [40606 'm.0_4pw_0002.png' 'Indian/m.0_4pw' ... 1 2 0]]\n",
      "12\n",
      "prediction_save_dir: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Predictions: 100%|██████████| 127/127 [00:03<00:00, 37.47batch/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from multi_head_resnet import MultiHeadResNet\n",
    "all_predictions = {}\n",
    "all_labels = {}\n",
    "all_file_names = {}\n",
    "attributes = ['eye_type', 'hair_color', 'hair_type', 'nose_type', 'skin_type']\n",
    "# attributes = ['eye_type']\n",
    "PRED_ROOT = '/media/global_data/fair_neural_compression_data/pred_with_raw_model_debug'# new folder \n",
    "# same <models> list for all the compression models.\n",
    "models = []\n",
    "for attribute in attributes:\n",
    "    # this dir is hardcoded as we store clean model training results here. \n",
    "    attribute_model_path = f'{PRED_ROOT}/hyperprior/celebA/clean/{attribute}_best.pth'\n",
    "    models.append(torch.load(attribute_model_path))\n",
    "\n",
    "\n",
    "# this is True if want to redo the evaluation. If False, we are loading from previous runs to save time. \n",
    "redo_evaluation = True\n",
    "\n",
    "if redo_evaluation:\n",
    "    for model_name in model_names:\n",
    "        print(model_name)\n",
    "        all_predictions[model_name] = {}\n",
    "        all_labels[model_name] = {}\n",
    "        all_file_names[model_name] = {}\n",
    "        for quality in qualities_dict[model_name]:\n",
    "            train_loader, valid_loader, test_loader = create_dataloaders(\n",
    "                f'{ROOT}/{model_name_dict[model_name]}/{dataset_name}/{quality}', \n",
    "                RFW_LABELS_DIR, \n",
    "                BATCH_SIZE, \n",
    "                RATIO\n",
    "            )\n",
    "            print(quality)\n",
    "            all_predictions[model_name][quality], \\\n",
    "            all_labels[model_name][quality], \\\n",
    "            all_file_names[model_name][quality] = \\\n",
    "                save_race_based_predictions(\n",
    "                    models,  \n",
    "                    test_loader, \n",
    "                    'cuda:1', \n",
    "                    \"\",\n",
    "                    attributes\n",
    "                )      \n",
    "else:\n",
    "    # load from saved files\n",
    "    # already have files, so load, instead of re-run the eval. \n",
    "    file = open(\"./raw_model_predictions.pkl\",'rb')\n",
    "    all_predictions = pickle.load(file)\n",
    "    file = open(\"./raw_model_labels.pkl\",'rb')\n",
    "    all_labels = pickle.load(file)\n",
    "    file = open(\"./raw_model_filenames.pkl\",'rb')\n",
    "    all_file_names = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save evaluation results. Uncomment when need to save evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction results\n",
    "with open('./raw_model_predictions.pkl', 'wb+') as f:\n",
    "    pickle.dump(all_predictions, f)\n",
    "with open('./raw_model_labels.pkl', 'wb+') as f:\n",
    "    pickle.dump(all_labels, f)\n",
    "with open('./raw_model_filenames.pkl', 'wb+') as f:\n",
    "    pickle.dump(all_file_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheng_preds, cheng_labels = all_predictions['cheng2020-attn'], all_labels['cheng2020-attn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheng_file_names = all_file_names['cheng2020-attn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2607346/1610086143.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.ne(torch.tensor(cheng_preds[quality][race][attribute]), torch.tensor(cheng_labels[quality][race][attribute]))\n"
     ]
    }
   ],
   "source": [
    "wrong_preds_file_names = {}\n",
    "\n",
    "for quality in qualities:\n",
    "    wrong_preds_file_names[quality] = {}\n",
    "    for race in RACE_LABELS:\n",
    "        wrong_preds_file_names[quality][race] = {}\n",
    "        for attribute in attributes:\n",
    "            mask = torch.ne(torch.tensor(cheng_preds[quality][race][attribute]), torch.tensor(cheng_labels[quality][race][attribute]))\n",
    "            wrong_preds_file_names[quality][race][attribute] = \\\n",
    "                np.array(cheng_file_names[quality][race][attribute])[np.array(torch.nonzero(mask).squeeze())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: cheng2020-attn\n",
      "dataset: celebA\n",
      "dataset: fairface\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_path = '/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64'\n",
    "bpp_data = {}\n",
    "datasets = ['celebA', 'fairface']\n",
    "for model_name in ['cheng2020-attn']:\n",
    "    if model_name == 'jpeg':\n",
    "        continue\n",
    "    print(f'model_name: {model_name}')\n",
    "    bpp_data[model_name] = {}\n",
    "    model_path = f'{data_path}/{model_name}'\n",
    "    for dataset in datasets:\n",
    "        print(f'dataset: {dataset}')\n",
    "        bpp_data[model_name][dataset] = {}\n",
    "        dataset_path = f'{model_path}/{dataset}'\n",
    "        for quality in qualities:\n",
    "            stats_path = f'{dataset_path}/{quality}/stats.json'\n",
    "            with open(stats_path, 'r') as json_file:\n",
    "                stats_data = json.load(json_file)\n",
    "            if \"results\" in stats_data:\n",
    "                bpp_data[model_name][dataset][quality] = stats_data['results']['bpp']\n",
    "            elif \"est_bpp\" in stats_data:\n",
    "                bpp_data[model_name][dataset][quality] = stats_data['est_bpp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941\n",
      "643\n",
      "258\n",
      "254\n",
      "200\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "# Tian: inspect African classification results\n",
    "race_of_interest = 'Asian'\n",
    "attribute_of_interest = 'eye_type'\n",
    "for quality in qualities:\n",
    "    mask = torch.ne(cheng_preds[quality][race_of_interest][attribute_of_interest], cheng_labels[quality][race_of_interest][attribute_of_interest])\n",
    "    wrong_preds_file_names[quality][race_of_interest][attribute_of_interest] = \\\n",
    "        np.array(cheng_file_names[quality][race_of_interest][attribute_of_interest])[np.array(torch.nonzero(mask).squeeze())]\n",
    "\n",
    "print(len(cheng_preds[quality][race_of_interest][attribute_of_interest]))\n",
    "\n",
    "for quality in qualities:\n",
    "    print(len(wrong_preds_file_names[quality][race_of_interest][attribute_of_interest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_of_interest = 'African'\n",
    "# attribute_of_interest = 'hair_type'\n",
    "\n",
    "def get_wrong_preds(race, attribute):\n",
    "    wrong_preds_file_names = {}\n",
    "\n",
    "    for quality in qualities:\n",
    "        wrong_preds_file_names[quality] = {}\n",
    "        for race_name in RACE_LABELS:\n",
    "            wrong_preds_file_names[quality][race_name] = {}\n",
    "            for attribute in attributes:\n",
    "                mask = torch.ne(cheng_preds[quality][race_name][attribute], cheng_labels[quality][race_name][attribute])\n",
    "                wrong_preds_file_names[quality][race_name][attribute] = \\\n",
    "                    np.array(cheng_file_names[quality][race_name][attribute])[np.array(torch.nonzero(mask).squeeze())]\n",
    "    wrong_preds_of_interest = {} # index: Last quality that was incorrect\n",
    "    for quality in qualities:\n",
    "        print(quality)\n",
    "        temp_wrong_preds = {}\n",
    "        new_wrong_preds = set(wrong_preds_file_names[quality][race][attribute])\n",
    "        wrong_preds_indices = set(wrong_preds_of_interest.keys())\n",
    "        print(len(wrong_preds_indices))\n",
    "        \n",
    "        wrong_preds_indices = list(new_wrong_preds.symmetric_difference(wrong_preds_indices))\n",
    "        for wrong_preds_index in wrong_preds_indices:\n",
    "            if wrong_preds_index in wrong_preds_of_interest:\n",
    "                temp_wrong_preds[wrong_preds_index] = wrong_preds_of_interest[wrong_preds_index]\n",
    "            else:\n",
    "                temp_wrong_preds[wrong_preds_index] = quality\n",
    "        wrong_preds_of_interest = temp_wrong_preds\n",
    "    return wrong_preds_of_interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds_of_interest = get_wrong_preds('Indian', 'skin_type')\n",
    "wrong_preds_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "filtered_keys = [key for key, value in wrong_preds_of_interest.items() if value != 'q_3']\n",
    "random.seed(42)\n",
    "sampled_keys = random.sample(filtered_keys, k=20)\n",
    "# print(selected_keys)\n",
    "\n",
    "for file_name in sampled_keys:\n",
    "    wrong_pred_quality = wrong_preds_of_interest[file_name]\n",
    "    image_files = []\n",
    "    for quality in qualities:\n",
    "        image_files.append(f'{ROOT}/{\"cheng2020-attn\"}/{dataset_name}/{quality}/{file_name}')\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        print(f'{image_files[i]}')\n",
    "        img = mpimg.imread(image_files[i])\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(qualities[i])\n",
    "        ax.axis('off')\n",
    "    fig.suptitle(f'Last incorrect classification: {wrong_pred_quality}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)  # Adjust the top to fit the suptitle\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFW_ROOT = '/media/global_data/fair_neural_compression_data/datasets/RFW/data_64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bpps = bpp_data['cheng2020-attn']['celebA']\n",
    "\n",
    "selected_images = [\n",
    "    'African/m.076yq88/m.076yq88_0001.png',\n",
    "    'Asian/m.01wh03h/m.01wh03h_0003.png',\n",
    "    'Caucasian/m.03plx_/m.03plx__0001.png',\n",
    "    'Indian/m.0k28ny5/m.0k28ny5_0002.png',\n",
    "]\n",
    "# Indian/m.0k28ny5/m.0k28ny5_0002.png # wrong only at q_0001\n",
    "races = ['African', 'Asian', 'Caucasian', 'Indian']\n",
    "race_attributes = ['hair_type', 'eye_type', 'hair_color', 'skin_type']\n",
    "\n",
    "mpl.style.use('seaborn-v0_8-colorblind')\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "fig, axes = plt.subplots(4, 6, figsize=(8, 6))\n",
    "plt.setp(axes, xticks=[], yticks=[], frame_on=False) # remove black borders, no xy axis\n",
    "for ax in axes:\n",
    "    ax[-1].axvline(x=0.5, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "for i, race_attribute in enumerate(race_attributes):\n",
    "    text_x = (len(qualities) + 4) / 2 - 0.5  # Centered text position\n",
    "    axes[i, 0].text(text_x, 1.15, ' '.join(race_attribute.split('_')).title(), ha='center', va='center', transform=axes[i, 0].transAxes)\n",
    "    \n",
    "for i, (file_name, race) in enumerate(zip(selected_images, races)):\n",
    "    wrong_preds_of_interest = get_wrong_preds(race, race_attributes[i])\n",
    "    print(race_attributes[i])\n",
    "    last_incorrect_quality = wrong_preds_of_interest[file_name]\n",
    "    last_incorrect_index = qualities.index(last_incorrect_quality)\n",
    "    for j in range(len(qualities) + 1):\n",
    "        if j == 0:\n",
    "            axes[i][j].set_ylabel(f'{race}')\n",
    "        if j == len(qualities):\n",
    "            image_path = f'{RFW_ROOT}/{file_name}'\n",
    "            if i == len(selected_images) - 1:\n",
    "                axes[i][j].set_xlabel(f'Original')\n",
    "        else:\n",
    "            color = 'r' if j <= last_incorrect_index else 'g'\n",
    "            rect = patches.Rectangle((0, 0), 63, 63, linewidth=7, edgecolor=color, facecolor='none')\n",
    "            axes[i][j].add_patch(rect)\n",
    "            image_path = f'{ROOT}/{\"cheng2020-attn\"}/{dataset_name}/{qualities[j]}/{file_name}'\n",
    "            if i == len(selected_images) - 1:\n",
    "                axes[i][j].set_xlabel(f'{\"{:.3f}\".format(bpps[qualities[j]])} bpp')\n",
    "        img = mpimg.imread(image_path)\n",
    "        axes[i][j].imshow(img)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.2, top=0.8, hspace=0.3)  # Adjust bottom and top margins as needed\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.1])\n",
    "# plt.savefig('/home/rasta/fair compression/fair-neural-compression-eval/plots/first_page_fig.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
