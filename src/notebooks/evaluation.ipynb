{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-v0_8-colorblind')\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"hyperprior\", \"mbt2018\", \"cheng2020-attn\", \"qres17m\", \"qarv\"]\n",
    "races = ['Indian', 'Asian', 'African', 'Caucasian']\n",
    "qualities = [\"q_0001\", \"q_0009\", \"q_1\", \"q_2\", \"q_3\"]\n",
    "categories = ['skin_type', 'eye_type', 'nose_type', 'lip_type', 'hair_type', 'hair_color']\n",
    "category_names = ['Skin Type', 'Eye Type', 'Nose Type', 'Lip Type', 'Hair Type', 'Hair Color']\n",
    "qres = [\"1\", \"3\", \"6\", \"9\", \"12\"]\n",
    "qarv = [\"lmb_1\", \"lmb_4\", \"lmb_8\", \"lmb_16\", \"lmb_32\" ]\n",
    "markers = ['s', 'o', '^', '*', 'D']\n",
    "linestyles = ['solid', 'dashed', 'dashdot', 'dotted', \"dotted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for a model and generate accuracies for all rates\n",
    "def generate_results(dataset=\"celebA\", model=\"hyperprior\"):\n",
    "    rates = []\n",
    "    results = {}\n",
    "    for i, q in enumerate(qualities):\n",
    "        results[q] = {}\n",
    "        #Iterate through qualities and pull dictionaries\n",
    "        with open(f'/media/global_data/fair_neural_compression_data/final_predictions/{model}/{dataset}/{q}/sep_predictions.pkl', 'rb') as f:\n",
    "            all_predictions = pickle.load(f)\n",
    "        with open(f'/media/global_data/fair_neural_compression_data/final_predictions/hyperprior/celebA/clean/sep_labels.pkl', 'rb') as f:\n",
    "            all_labels = pickle.load(f)\n",
    "        \n",
    "        if model == \"qarv\":\n",
    "            with open(f'/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/qarv/celebA/{qarv[i]}/stats.json', 'r') as json_file:\n",
    "                        data_dict = json.load(json_file)\n",
    "            bpp = data_dict['results']['bpp']\n",
    "            pass\n",
    "        elif model == \"qres17m\":\n",
    "            with open(f'/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/qres17m_lmb_64/celebA/{qres[i]}/stats.json', 'r') as json_file:\n",
    "                        data_dict = json.load(json_file)\n",
    "            bpp = data_dict['results']['bpp']\n",
    "        else:\n",
    "            with open(f'/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/{model}/{dataset}/{q}/stats.json', 'r') as json_file:\n",
    "                        data_dict = json.load(json_file)\n",
    "            bpp = data_dict['est_bpp']\n",
    "        rates.append(bpp)\n",
    "        for race in races:\n",
    "            results[q][race] = {}\n",
    "            for cat in categories:\n",
    "                if cat == 'skin_type':\n",
    "                    pass\n",
    "                pred = all_predictions[race][cat]\n",
    "                labels = all_labels[race][cat]\n",
    "                score = accuracy_score(pred, labels)\n",
    "                results[q][race][cat] = score\n",
    "                \n",
    "        \n",
    "    temp = {}\n",
    "    for cat in categories:\n",
    "        temp[cat] = {}\n",
    "        for race in races:\n",
    "            temp[cat][race] = [results[q][race][cat] for q in qualities]\n",
    "    return temp, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/media/global_data/fair_neural_compression_data/predictions/hyperprior/celebA/clean/sep_labels.pkl', 'rb') as f:\n",
    "     pred_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for race in races:\n",
    "    counts.append(len(pred_dict[race][\"eye_type\"]))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fairface\"\n",
    "fig, axes = plt.subplots(len(categories)//2, 2, figsize=(6, 9))\n",
    "\n",
    "\n",
    "## Get rates\n",
    "for z, model in enumerate(models):\n",
    "    temp, rates = generate_results(model=model)\n",
    "    for i, cat in enumerate(categories):\n",
    "        accs = []\n",
    "        axes[i%3][i//3].set_title(category_names[i])\n",
    "        for j, q in enumerate(qualities):\n",
    "            yurg = []\n",
    "            for race in races:\n",
    "                yurg.append(temp[cat][race][j])\n",
    "           \n",
    "            accs.append(np.average(yurg, weights=counts))\n",
    "        axes[i%3][i//3].plot(rates, accs, label = model, linestyle = linestyles[z], marker = markers[z])\n",
    "        axes[2][i//3].set_xlabel(\"bpp\")\n",
    "        axes[i%3][0].set_ylabel(\"Accuracy\")\n",
    "        \n",
    "axes[2][1].legend(bbox_to_anchor=(0.7, -0.2), ncol = 3)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "\n",
    "axes[0][0].set_ylim(0.37, 0.57)\n",
    "axes[0][1].set_ylim(0.62, 0.82)\n",
    "\n",
    "\n",
    "axes[1][0].set_ylim(0.75, 0.95)\n",
    "axes[1][1].set_ylim(0.55, 0.75)\n",
    "\n",
    "axes[2][0].set_ylim(0.55, 0.75)\n",
    "axes[2][1].set_ylim(0.62, 0.82)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Weighted average across races\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = ['Indian', 'Asian', 'African', 'Caucasian']\n",
    "categories = ['skin_type', 'eye_type', 'nose_type', 'lip_type', 'hair_type', 'hair_color']\n",
    "category_names = ['Skin Type', 'Eye Type', 'Nose Type', 'Lip Type', 'Hair Type', 'Hair Color']\n",
    "lambda_file_names = [\"1\"]\n",
    "lambda_values = [1]\n",
    "data_rate_values = [\"clean\"]\n",
    "qualities = [\"q_0001\", \"q_0009\", \"q_1\", \"q_2\", \"q_3\"]\n",
    "qres = [\"1\", \"3\", \"6\", \"9\", \"12\"]\n",
    "qarv = [\"lmb_1\", \"lmb_4\", \"lmb_8\", \"lmb_16\", \"lmb_32\" ]\n",
    "rates = []\n",
    "race_markers = ['s', 'o', '^', '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for a model and generate accuracies for all rates\n",
    "def generate_results(dataset=\"celebA\", model=\"hyperprior\"):\n",
    "    rates = []\n",
    "    results = {}\n",
    "    for i, q in enumerate(qualities):\n",
    "        results[q] = {}\n",
    "        #Iterate through qualities and pull dictionaries\n",
    "        with open(f'/media/global_data/fair_neural_compression_data/final_predictions/{model}/{dataset}/{q}/sep_predictions.pkl', 'rb') as f:\n",
    "            all_predictions = pickle.load(f)\n",
    "        with open(f'/media/global_data/fair_neural_compression_data/final_predictions/hyperprior/celebA/clean/sep_labels.pkl', 'rb') as f:\n",
    "            all_labels = pickle.load(f)\n",
    "\n",
    "        if model == \"qarv\":\n",
    "            with open(f'/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/qarv/celebA/{qarv[i]}/stats.json', 'r') as json_file:\n",
    "                        data_dict = json.load(json_file)\n",
    "            bpp = data_dict['results']['bpp']\n",
    "\n",
    "        elif model == \"qres17m\":\n",
    "            with open(f'/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/qres17m_lmb_64/celebA/{qres[i]}/stats.json', 'r') as json_file:\n",
    "                        data_dict = json.load(json_file)\n",
    "            bpp = data_dict['results']['bpp']\n",
    "        else:\n",
    "            with open(f'/media/global_data/fair_neural_compression_data/decoded_rfw/decoded_64x64/{model}/{dataset}/{q}/stats.json', 'r') as json_file:\n",
    "                        data_dict = json.load(json_file)\n",
    "            bpp = data_dict['est_bpp']\n",
    "        rates.append(bpp)\n",
    "        merged_skin_type={\n",
    "              'African':(5, 4), \n",
    "              'Asian':(3, 2), \n",
    "              'Caucasian':(2, 1), \n",
    "              'Indian':(3, 2)\n",
    "        }\n",
    "        for race in races:\n",
    "            results[q][race] = {}\n",
    "            for cat in categories:\n",
    "                pred = all_predictions[race][cat]\n",
    "                labels = all_labels[race][cat]\n",
    "                if cat == 'skin_type':\n",
    "                    pred[pred==merged_skin_type[race][0]] = merged_skin_type[race][1]\n",
    "                    labels[labels==merged_skin_type[race][0]] = merged_skin_type[race][1]\n",
    "                    if race ==\"Caucasian\":\n",
    "                        pred[pred==3] = 1\n",
    "                        labels[labels==3] = 1                       \n",
    "                    print(max(labels), race, q)\n",
    "                    print(confusion_matrix(labels, pred, labels=[0, 1, 2, 3, 4, 5]))\n",
    "                    # pass\n",
    "                score = accuracy_score(pred, labels)\n",
    "                results[q][race][cat] = score\n",
    "                # print(q, race, cat)\n",
    "                # print(confusion_matrix(labels, pred))\n",
    "        \n",
    "    temp = {}\n",
    "    for cat in categories:\n",
    "        temp[cat] = {}\n",
    "        for race in races:\n",
    "            temp[cat][race] = [results[q][race][cat] for q in qualities]\n",
    "    return temp, rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_pred(dataset=\"celebA\", model=\"hyperprior\"):\n",
    "    with open(f'/media/global_data/fair_neural_compression_data/final_predictions/{model}/{dataset}/clean/sep_predictions.pkl', 'rb') as f:\n",
    "                all_predictions = pickle.load(f)\n",
    "    with open(f'/media/global_data/fair_neural_compression_data/final_predictions/hyperprior/celebA/clean/sep_labels.pkl', 'rb') as f:\n",
    "                all_labels = pickle.load(f)\n",
    "    results = {}\n",
    "    merged_skin_type={\n",
    "            'African':(5, 4), \n",
    "            'Asian':(3, 2), \n",
    "            'Caucasian':(2, 1), \n",
    "            'Indian':(3, 2)\n",
    "    }\n",
    "    for race in races:\n",
    "            results[race] = {}\n",
    "            for cat in categories:\n",
    "                pred = all_predictions[race][cat]\n",
    "                labels = all_labels[race][cat]\n",
    "                if cat == 'skin_type':\n",
    "                    pred[pred==merged_skin_type[race][0]] = merged_skin_type[race][1]\n",
    "                    labels[labels==merged_skin_type[race][0]] = merged_skin_type[race][1]\n",
    "                    if race ==\"Caucasian\":\n",
    "                        pred[pred==3] = 1\n",
    "                        labels[labels==3] = 1 \n",
    "                results[race][cat] = accuracy_score(pred, labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mbt2018\"\n",
    "\n",
    "temp, rates = generate_results(model=model)\n",
    "ff_temp, ff_rates = generate_results(\"fairface\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_clean_pred()\n",
    "ff_results = get_clean_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-v0_8-colorblind')\n",
    "#plt.rcParams[\"font.family\"] = \"lucida-console\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_races_and_fairness(temp, rates, results, trained_on, figure_name, write_image=False):\n",
    "    fig, axes = plt.subplots(len(categories)//2, 2, figsize=(6, 9), sharey=True)\n",
    "    \n",
    "    #colors = ['b', 'y', 'g', 'r']\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        zz = []\n",
    "        axes[i%3][i//3].set_title(category_names[i])\n",
    "        for j, race in enumerate(races):\n",
    "            axes[i%3][i//3].plot(rates, temp[cat][race], marker=race_markers[j], label=race,) #color = colors[j],\n",
    "            zz.append(temp[cat][race])\n",
    "        #print(zz)\n",
    "        axes[i%3][i//3].plot(rates, np.array(zz).max(0) - np.array(zz).min(0), marker=race_markers[j], color = 'black', label=\"Fairness\")\n",
    "        print(np.array(zz).max(0) - np.array(zz).min(0))\n",
    "\n",
    "    for j, race in enumerate(races):\n",
    "        for i, cat in enumerate(categories):\n",
    "            axes[i%3][i//3].axhline(results[race][cat], linestyle=\"--\", color=plt.rcParams['axes.prop_cycle'].by_key()['color'][j] )#color=colors[j])\n",
    "    axes[2][1].legend(bbox_to_anchor=(0.7, -0.2), ncol = 3)\n",
    "    plt.suptitle(model)\n",
    "    if write_image:\n",
    "        fig.savefig(os.path.join('../../plots/new_training_merge_skintype', model, f'{model}_{trained_on}_{figure_name}.png'), dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_races_and_fairness(temp, rates, results, 'celeba', 'racial_disparity', write_image=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_races_and_fairness(ff_temp, ff_rates, ff_results, 'fairface', 'racial_disparity', write_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_races_and_fairness_single(temp, rates, results):\n",
    "    categories = [\"eye_type\"]\n",
    "    fig, axes = plt.subplots(len(categories), figsize=(3.3, 3.3), sharey=True)\n",
    "    \n",
    "    colors = ['b', 'y', 'g', 'r']\n",
    "    axes.set_title(\"Accuracy vs Rate (by Group)\")\n",
    "    for i, cat in enumerate(categories):\n",
    "        zz = []\n",
    "\n",
    "        for j, race in enumerate(races):\n",
    "            axes.plot(rates, temp[cat][race], marker=race_markers[j], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][j], label=race)\n",
    "\n",
    "            zz.append(temp[cat][race])\n",
    "        axes.plot(rates, np.array(zz).max(0) - np.array(zz).min(0), marker=\"d\", linestyle = \"dashdot\" , color = plt.rcParams['axes.prop_cycle'].by_key()['color'][5], label=\"bias\")\n",
    "        #print(np.array(zz).max(0) - np.array(zz).min(0))\n",
    "\n",
    "    for j, race in enumerate(races):\n",
    "        for i, cat in enumerate(categories):\n",
    "            axes.axhline(results[race][cat], linestyle=\"--\", color=plt.rcParams['axes.prop_cycle'].by_key()['color'][j])\n",
    "    axes.legend(bbox_to_anchor=(1.6, 0.7))\n",
    "    axes.set_xlabel(\"bpp\")\n",
    "    axes.set_ylabel(\"Accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_races_and_fairness_single(temp, rates, results)\n",
    "# todo: keep which??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_fairness(temp, rates, trained_on, write_image=False):\n",
    "    fig, axes = plt.subplots(1, figsize=(3.3, 3.3), sharey=True)\n",
    "\n",
    "    #colors = ['b', 'y', 'g', 'r', \"darkviolet\", \"slategrey\"]\n",
    "    markers = ['s', 'o', '^', '*', \"v\", \"h\"]\n",
    "\n",
    "    axes.set_title(f\"Fairness, {model}, {trained_on}\")\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        zz = []\n",
    "        for j, race in enumerate(races):\n",
    "            #axes[i].plot(rates, temp[cat][race], marker=race_markers[j], color = colors[j], label=race)\n",
    "            zz.append(temp[cat][race])\n",
    "        axes.plot(rates, np.array(zz).max(0) - np.array(zz).min(0), marker=markers[i], label=cat)\n",
    "    axes.legend(bbox_to_anchor=(1.05, 0.7))\n",
    "    axes.set_xlabel(\"bpp\")\n",
    "    axes.set_ylabel(\"bias\")\n",
    "\n",
    "    #for j, race in enumerate(races):\n",
    "    #    for i, cat in enumerate(categories):\n",
    "    #        axes[i].axhline(results[race][cat], linestyle=\"--\", color=colors[j])\n",
    "    figure_name= 'all_fairness'\n",
    "    if write_image:\n",
    "        fig.savefig(os.path.join('../../plots/new_training_merge_skintype', model, f'{model}_{trained_on}_{figure_name}.png'), bbox_inches='tight', dpi=200)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fairness(temp, rates, 'celeba', write_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fairness(ff_temp, ff_rates, 'fairface', write_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fairness_comparison(temp, ff_temp, rates, write_image=False):\n",
    "    categories = [\"skin_type\", \"eye_type\", \"hair_color\"]\n",
    "    fig, axes = plt.subplots(1, figsize=(3.3, 3.3), sharey=True)\n",
    "    markers = ['s', 'o', '^', '*', \"v\", \"D\"]\n",
    "\n",
    "    axes.set_title(f\"FairFace/celebA Comparison, {model}\")\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        zz = []\n",
    "        yy = []\n",
    "        for j, race in enumerate(races):\n",
    "            #axes[i].plot(rates, temp[cat][race], marker=race_markers[j], color = colors[j], label=race)\n",
    "            zz.append(temp[cat][race])\n",
    "            yy.append(ff_temp[cat][race])\n",
    "        axes.plot(rates, np.array(zz).max(0) - np.array(zz).min(0), marker=markers[i],  linestyle = \"-\", label=f\"{cat} (celebA)\", color=plt.rcParams['axes.prop_cycle'].by_key()['color'][i])\n",
    "        axes.plot(rates, np.array(yy).max(0) - np.array(yy).min(0), marker=markers[i], linestyle = \"--\", label=f\"{cat} (Fairface)\", color = plt.rcParams['axes.prop_cycle'].by_key()['color'][i])\n",
    "    axes.legend(bbox_to_anchor=(1.05, 0.7))\n",
    "    axes.set_xlabel(\"bpp\")\n",
    "    axes.set_ylabel(\"bias\")\n",
    "        #print(np.array(zz).max(0) - np.array(zz).min(0))\n",
    "    \n",
    "        \n",
    "    #for i, cat in enumerate(categories):\n",
    "       # max_acc = 0\n",
    "       # min_acc = 1\n",
    "       # for j, race in enumerate(races):\n",
    "       #     value = results[race][cat]\n",
    "       #     if value > max_acc:\n",
    "                #max_acc = value\n",
    "       #     if value < min_acc:\n",
    "                #min_acc = value\n",
    "        #axes[i].axhline(max_acc - min_acc, linestyle=\"--\", color=colors[j])\n",
    "    if write_image:\n",
    "        fig.savefig(os.path.join('../../plots/new_training_merge_skintype', model, f'{model}_dataset_comp.png'), bbox_inches='tight', dpi=200)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fairness(temp, ff_temp): \n",
    "    model_fairness = {}\n",
    "    model_fairness[\"celebA\"] = {}\n",
    "    model_fairness[\"fairface\"] = {}\n",
    "    for i, cat in enumerate(categories):\n",
    "        zz = []\n",
    "        yy = []\n",
    "        for j, race in enumerate(races):\n",
    "            zz.append(temp[cat][race])\n",
    "            yy.append(ff_temp[cat][race])\n",
    "        model_fairness[\"celebA\"][cat] = (np.array(zz).max(0) - np.array(zz).min(0)).tolist()\n",
    "        model_fairness[\"fairface\"][cat] = (np.array(yy).max(0) - np.array(yy).min(0)).tolist()\n",
    "    return model_fairness\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_list = [\"hyperprior\", \"qres17m\", \"cheng2020-attn\", \"mbt2018\", \"qarv\"]\n",
    "\n",
    "#all_fairness = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in model_list:\n",
    "#    temp, rates = generate_results(model=model)\n",
    "#    ff_temp, ff_rates = generate_results(\"fairface\", model=model)\n",
    "#    all_fairness[model] = compute_fairness(temp, ff_temp)\n",
    "#all_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dump Fairness value to json\n",
    "\n",
    "#import json\n",
    "#with open('sep_fairness.json', 'w') as fp:\n",
    "#    json.dump(all_fairness, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(temp, ff_temp, rates, write_image=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nc_sandbox",
   "language": "python",
   "name": "neural_comp_sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
